{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e458a9",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    hamming_loss, accuracy_score, classification_report\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Seed pour reproductibilitÃ©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"âœ… Imports rÃ©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01bcde",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 2. CrÃ©ation des Dossiers et Montage Google Drive (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les dossiers nÃ©cessaires\n",
    "folders = [\n",
    "    'data/processed',\n",
    "    'models/cnn_bilstm',\n",
    "    'results/figures',\n",
    "    'results/metrics'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"âœ… Dossiers crÃ©Ã©s!\")\n",
    "\n",
    "# Optionnel: Monter Google Drive pour sauvegarder les rÃ©sultats\n",
    "MOUNT_DRIVE = False  # Mettre Ã  True pour monter Google Drive\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive montÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e6ea3",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 3. Chargement des DonnÃ©es PrÃ©parÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11040031",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chargement des donnÃ©es...\\n\")\n",
    "\n",
    "# Charger les sÃ©quences\n",
    "X_train = np.load('data/processed/X_train.npy')\n",
    "X_val = np.load('data/processed/X_val.npy')\n",
    "X_test = np.load('data/processed/X_test.npy')\n",
    "print(\"âœ… SÃ©quences chargÃ©es\")\n",
    "\n",
    "# Charger les labels\n",
    "y_train = np.load('data/processed/y_train.npy')\n",
    "y_val = np.load('data/processed/y_val.npy')\n",
    "y_test = np.load('data/processed/y_test.npy')\n",
    "print(\"âœ… Labels chargÃ©s\")\n",
    "\n",
    "# Charger le tokenizer\n",
    "with open('data/processed/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "print(\"âœ… Tokenizer chargÃ©\")\n",
    "\n",
    "# Charger les mÃ©tadonnÃ©es\n",
    "with open('data/processed/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "VOCAB_SIZE = metadata['vocab_size']\n",
    "MAX_LENGTH = metadata['max_length']\n",
    "NUM_CLASSES = metadata['num_classes']\n",
    "EMOTION_LABELS = metadata['emotion_labels']\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistiques:\")\n",
    "print(f\"  Taille vocabulaire: {VOCAB_SIZE:,}\")\n",
    "print(f\"  Longueur maximale: {MAX_LENGTH}\")\n",
    "print(f\"  Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"  Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a81d2",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ 4. DÃ©finition de la Couche d'Attention\n",
    "\n",
    "RÃ©utilisation de la mÃªme couche d'attention que dans le modÃ¨le BiLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752acddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche d'attention personnalisÃ©e\n",
    "    \n",
    "    Cette couche calcule un score d'attention pour chaque timestep,\n",
    "    puis applique une somme pondÃ©rÃ©e sur les sorties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, timesteps, features)\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, timesteps, features)\n",
    "        \n",
    "        # Calculer les scores d'attention\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # Appliquer softmax pour obtenir les poids d'attention\n",
    "        a = K.softmax(e, axis=1)\n",
    "        \n",
    "        # Appliquer les poids d'attention\n",
    "        output = x * a\n",
    "        output = K.sum(output, axis=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "print(\"âœ… Couche d'attention dÃ©finie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed09a1",
   "metadata": {},
   "source": [
    "## ðŸ”¨ 5. Construction du ModÃ¨le CNN-BiLSTM avec Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e29029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_bilstm_attention_model(vocab_size, max_length, num_classes, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    CrÃ©e un modÃ¨le hybride CNN-BiLSTM avec attention\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding\n",
    "    - 3 branches CNN parallÃ¨les (kernel sizes: 3, 4, 5)\n",
    "    - Concatenation des CNN\n",
    "    - BiLSTM (128 units)\n",
    "    - Attention Layer\n",
    "    - Dense layers avec dropout\n",
    "    - Dense(num_classes, sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input\n",
    "    inputs = layers.Input(shape=(max_length,), name='input')\n",
    "    \n",
    "    # Embedding\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_length,\n",
    "        name='embedding'\n",
    "    )(inputs)\n",
    "    \n",
    "    # Dropout aprÃ¨s embedding\n",
    "    embedding = layers.Dropout(0.2)(embedding)\n",
    "    \n",
    "    # === CNN Branches ===\n",
    "    # Plusieurs tailles de noyaux pour capturer diffÃ©rents n-grammes\n",
    "    \n",
    "    # Branch 1: Kernel size 3 (trigrams)\n",
    "    conv_3 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_3'\n",
    "    )(embedding)\n",
    "    conv_3 = layers.MaxPooling1D(pool_size=2, name='maxpool_3')(conv_3)\n",
    "    \n",
    "    # Branch 2: Kernel size 4 (4-grams)\n",
    "    conv_4 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=4,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_4'\n",
    "    )(embedding)\n",
    "    conv_4 = layers.MaxPooling1D(pool_size=2, name='maxpool_4')(conv_4)\n",
    "    \n",
    "    # Branch 3: Kernel size 5 (5-grams)\n",
    "    conv_5 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=5,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_5'\n",
    "    )(embedding)\n",
    "    conv_5 = layers.MaxPooling1D(pool_size=2, name='maxpool_5')(conv_5)\n",
    "    \n",
    "    # Concatener les sorties des CNN\n",
    "    cnn_concat = layers.Concatenate(name='cnn_concat')([conv_3, conv_4, conv_5])\n",
    "    cnn_concat = layers.Dropout(0.3)(cnn_concat)\n",
    "    \n",
    "    # === BiLSTM Layer ===\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        name='bilstm'\n",
    "    )(cnn_concat)\n",
    "    \n",
    "    # === Attention Layer ===\n",
    "    attention_output = AttentionLayer(name='attention')(bilstm)\n",
    "    \n",
    "    # === Dense Layers ===\n",
    "    x = layers.Dense(256, activation='relu', name='dense_1')(attention_output)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', name='dense_3')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer (sigmoid pour multi-label)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # CrÃ©er le modÃ¨le\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='CNN_BiLSTM_Attention')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# CrÃ©er le modÃ¨le\n",
    "model = create_cnn_bilstm_attention_model(VOCAB_SIZE, MAX_LENGTH, NUM_CLASSES)\n",
    "\n",
    "# Compiler le modÃ¨le\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()\n",
    "\n",
    "# Compter les paramÃ¨tres\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nðŸ“Š Total paramÃ¨tres: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e584f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 6. Configuration des Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les callbacks\n",
    "callbacks = [\n",
    "    # Early stopping: arrÃªter si pas d'amÃ©lioration aprÃ¨s 5 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint: sauvegarder le meilleur modÃ¨le\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_bilstm/best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate: rÃ©duire le LR si plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6cbe86",
   "metadata": {},
   "source": [
    "## ðŸš€ 7. EntraÃ®nement du ModÃ¨le\n",
    "\n",
    "â±ï¸ **Temps estimÃ©**: 40-50 minutes sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ DÃ©but de l'entraÃ®nement...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nâœ… EntraÃ®nement terminÃ©!\")\n",
    "print(f\"â±ï¸ Temps d'entraÃ®nement: {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e083bb",
   "metadata": {},
   "source": [
    "## ðŸ“Š 8. Visualisation de l'EntraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une figure avec 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "axes[1, 0].set_title('Precision Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
    "axes[1, 1].set_title('Recall Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/cnn_bilstm_attention_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702ece4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 9. Ã‰valuation sur l'Ensemble de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Ã‰valuation sur l'ensemble de test...\\n\")\n",
    "\n",
    "# PrÃ©dictions\n",
    "y_pred_proba = model.predict(X_test, batch_size=128, verbose=1)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculer les mÃ©triques\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "subset_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š RÃ‰SULTATS SUR L'ENSEMBLE DE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸŽ¯ MÃ©triques Globales:\")\n",
    "print(f\"  Precision (micro): {precision_micro:.4f}\")\n",
    "print(f\"  Precision (macro): {precision_macro:.4f}\")\n",
    "print(f\"  Recall (micro):    {recall_micro:.4f}\")\n",
    "print(f\"  Recall (macro):    {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score (micro):  {f1_micro:.4f}\")\n",
    "print(f\"  F1-Score (macro):  {f1_macro:.4f}\")\n",
    "print(f\"  Hamming Loss:      {hamming:.4f}\")\n",
    "print(f\"  Subset Accuracy:   {subset_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f9e78",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 10. MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les mÃ©triques par classe\n",
    "precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# CrÃ©er un DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Emotion': EMOTION_LABELS,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class,\n",
    "    'Support': y_test.sum(axis=0)\n",
    "})\n",
    "\n",
    "# Trier par F1-Score\n",
    "metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š TOP 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š BOTTOM 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "metrics_df.to_csv('results/metrics/cnn_bilstm_attention_per_class.csv', index=False)\n",
    "print(\"\\nâœ… MÃ©triques par classe sauvegardÃ©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7b9bc",
   "metadata": {},
   "source": [
    "## ðŸ“Š 11. Visualisation des MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les 15 meilleures et 15 pires Ã©motions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Top 15\n",
    "top_15 = metrics_df.head(15).sort_values('F1-Score')\n",
    "axes[0].barh(top_15['Emotion'], top_15['F1-Score'], color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Top 15 Ã‰motions (Meilleures F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom 15\n",
    "bottom_15 = metrics_df.tail(15).sort_values('F1-Score')\n",
    "axes[1].barh(bottom_15['Emotion'], bottom_15['F1-Score'], color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('Bottom 15 Ã‰motions (Pires F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/cnn_bilstm_attention_per_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51812b",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 12. Sauvegarde des RÃ©sultats Complets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les rÃ©sultats JSON\n",
    "results = {\n",
    "    'model_name': 'CNN-BiLSTM-Attention',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'training_time_minutes': training_time / 60,\n",
    "    'total_params': int(total_params),\n",
    "    'metrics': {\n",
    "        'precision_micro': float(precision_micro),\n",
    "        'precision_macro': float(precision_macro),\n",
    "        'recall_micro': float(recall_micro),\n",
    "        'recall_macro': float(recall_macro),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'subset_accuracy': float(subset_acc)\n",
    "    },\n",
    "    'best_epoch': int(np.argmin(history.history['val_loss'])) + 1,\n",
    "    'best_val_loss': float(min(history.history['val_loss']))\n",
    "}\n",
    "\n",
    "with open('results/metrics/cnn_bilstm_attention_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… RÃ©sultats sauvegardÃ©s dans results/metrics/cnn_bilstm_attention_results.json\")\n",
    "\n",
    "# Sauvegarder l'historique\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('results/metrics/cnn_bilstm_attention_history.csv', index=False)\n",
    "print(\"âœ… Historique sauvegardÃ©!\")\n",
    "\n",
    "# Sauvegarder les prÃ©dictions\n",
    "np.save('results/metrics/cnn_bilstm_attention_predictions.npy', y_pred_proba)\n",
    "print(\"âœ… PrÃ©dictions sauvegardÃ©es!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ NOTEBOOK CNN-BiLSTM-ATTENTION TERMINÃ‰!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
