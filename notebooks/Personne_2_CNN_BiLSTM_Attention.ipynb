{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e458a9",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    hamming_loss, accuracy_score, classification_report,\n",
    "    roc_auc_score, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Seed pour reproductibilitÃ©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"âœ… Imports rÃ©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01bcde",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 2. CrÃ©ation des Dossiers et Montage Google Drive (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les dossiers nÃ©cessaires\n",
    "folders = [\n",
    "    'data/processed',\n",
    "    'models/cnn_bilstm',\n",
    "    'results/figures',\n",
    "    'results/metrics'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"âœ… Dossiers crÃ©Ã©s!\")\n",
    "\n",
    "# Optionnel: Monter Google Drive pour sauvegarder les rÃ©sultats\n",
    "MOUNT_DRIVE = False  # Mettre Ã  True pour monter Google Drive\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive montÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e6ea3",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 3. Chargement des DonnÃ©es PrÃ©parÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11040031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Chargement des donnÃ©es depuis Google Drive...\\n\")\n",
    "\n",
    "# DÃ©finir le chemin vers les donnÃ©es dans Drive\n",
    "DATA_PATH = '/content/drive/MyDrive/emotion_detection_project/processed'\n",
    "\n",
    "# Charger les sÃ©quences\n",
    "X_train = np.load(f'{DATA_PATH}/X_train.npy')\n",
    "X_val = np.load(f'{DATA_PATH}/X_val.npy')\n",
    "X_test = np.load(f'{DATA_PATH}/X_test.npy')\n",
    "print(\"âœ… SÃ©quences chargÃ©es\")\n",
    "\n",
    "# Charger les labels\n",
    "y_train = np.load(f'{DATA_PATH}/y_train.npy')\n",
    "y_val = np.load(f'{DATA_PATH}/y_val.npy')\n",
    "y_test = np.load(f'{DATA_PATH}/y_test.npy')\n",
    "print(\"âœ… Labels chargÃ©s\")\n",
    "\n",
    "# Charger le tokenizer\n",
    "with open(f'{DATA_PATH}/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "print(\"âœ… Tokenizer chargÃ©\")\n",
    "\n",
    "# Charger les mÃ©tadonnÃ©es\n",
    "with open(f'{DATA_PATH}/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "VOCAB_SIZE = metadata['vocab_size']\n",
    "MAX_LENGTH = metadata['max_length']\n",
    "NUM_CLASSES = metadata['num_classes']\n",
    "EMOTION_LABELS = metadata['emotion_labels']\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistiques:\")\n",
    "print(f\"  Taille vocabulaire: {VOCAB_SIZE:,}\")\n",
    "print(f\"  Longueur maximale: {MAX_LENGTH}\")\n",
    "print(f\"  Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"  Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\")\n",
    "\n",
    "# Charger la matrice d'embedding (si disponible) - Pour Partie 4 (Study d'Ablation)\n",
    "try:\n",
    "    if os.path.exists(f'{DATA_PATH}/embedding_matrix.npy'):\n",
    "        embedding_matrix = np.load(f'{DATA_PATH}/embedding_matrix.npy')\n",
    "        USE_GLOVE = True\n",
    "        print(f\"âœ… Matrice d'embedding GloVe chargÃ©e: {embedding_matrix.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    embedding_matrix = None\n",
    "    USE_GLOVE = False\n",
    "    print(\"âš ï¸ Matrice d'embedding non trouvÃ©e. EntraÃ®nement ex-nihilo.\")\n",
    "\n",
    "# Charger les poids de classes - Pour Partie 1 (DÃ©sÃ©quilibre)\n",
    "try:\n",
    "    if os.path.exists(f'{DATA_PATH}/class_weights.pkl'):\n",
    "        with open(f'{DATA_PATH}/class_weights.pkl', 'rb') as f:\n",
    "            class_weights = pickle.load(f)\n",
    "        print(f\"âœ… Poids de classes chargÃ©s (Top 1: {list(class_weights.values())[0]:.2f})\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    class_weights = None\n",
    "    print(\"âš ï¸ Poids de classes non trouvÃ©s.\")\n",
    "\n",
    "# Charger la matrice d'embedding (si disponible) - Pour Partie 4 (Study d'Ablation)\n",
    "try:\n",
    "    embedding_matrix = np.load(f'{DATA_PATH}/embedding_matrix.npy')\n",
    "    USE_GLOVE = True\n",
    "    print(f\"âœ… Matrice d'embedding GloVe chargÃ©e: {embedding_matrix.shape}\")\n",
    "except FileNotFoundError:\n",
    "    embedding_matrix = None\n",
    "    USE_GLOVE = False\n",
    "    print(\"âš ï¸ Matrice d'embedding non trouvÃ©e. EntraÃ®nement ex-nihilo.\")\n",
    "\n",
    "# Charger les poids de classes - Pour Partie 1 (DÃ©sÃ©quilibre)\n",
    "try:\n",
    "    with open(f'{DATA_PATH}/class_weights.pkl', 'rb') as f:\n",
    "        class_weights = pickle.load(f)\n",
    "    print(f\"âœ… Poids de classes chargÃ©s (Top 1: {list(class_weights.values())[0]:.2f})\")\n",
    "except FileNotFoundError:\n",
    "    class_weights = None\n",
    "    print(\"âš ï¸ Poids de classes non trouvÃ©s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a81d2",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ 4. DÃ©finition de la Couche d'Attention\n",
    "\n",
    "RÃ©utilisation de la mÃªme couche d'attention que dans le modÃ¨le BiLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752acddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche d'attention personnalisÃ©e\n",
    "    \n",
    "    Cette couche calcule un score d'attention pour chaque timestep,\n",
    "    puis applique une somme pondÃ©rÃ©e sur les sorties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch_size, timesteps, features)\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, timesteps, features)\n",
    "        \n",
    "        # Calculer les scores d'attention\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # Appliquer softmax pour obtenir les poids d'attention\n",
    "        a = K.softmax(e, axis=1)\n",
    "        \n",
    "        # Appliquer les poids d'attention\n",
    "        output = x * a\n",
    "        output = K.sum(output, axis=1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "print(\"âœ… Couche d'attention dÃ©finie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed09a1",
   "metadata": {},
   "source": [
    "## ðŸ”¨ 5. Construction du ModÃ¨le CNN-BiLSTM avec Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e29029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_bilstm_attention_model(vocab_size, max_length, num_classes, embedding_dim=128, embedding_matrix=None):\n",
    "    \"\"\"\n",
    "    CrÃ©e un modÃ¨le hybride CNN-BiLSTM avec attention\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding (Glove ou Ex-Nihilo)\n",
    "    - 3 branches CNN parallÃ¨les (kernel sizes: 3, 4, 5)\n",
    "    - Concatenation des CNN\n",
    "    - BiLSTM (128 units)\n",
    "    - Attention Layer\n",
    "    - Dense layers avec dropout\n",
    "    - Dense(num_classes, sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input\n",
    "    inputs = layers.Input(shape=(max_length,), name='input')\n",
    "    \n",
    "    # Embedding\n",
    "    if embedding_matrix is not None:\n",
    "        print(\"ðŸ’¡ Utilisation des embeddings prÃ©-entraÃ®nÃ©s GloVe\")\n",
    "        embedding = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_length,\n",
    "            trainable=False,  # On gÃ¨le les poids pour le dÃ©but (transfer learning)\n",
    "            name='embedding_glove'\n",
    "        )(inputs)\n",
    "    else:\n",
    "        print(\"ðŸ’¡ Utilisation des embeddings appris (Ex-Nihilo)\")\n",
    "        embedding = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length,\n",
    "            name='embedding'\n",
    "        )(inputs)\n",
    "    \n",
    "    # Dropout aprÃ¨s embedding\n",
    "    embedding = layers.Dropout(0.2)(embedding)\n",
    "    \n",
    "    # === CNN Branches ===\n",
    "    # Plusieurs tailles de noyaux pour capturer diffÃ©rents n-grammes\n",
    "    \n",
    "    # Branch 1: Kernel size 3 (trigrams)\n",
    "    conv_3 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_3'\n",
    "    )(embedding)\n",
    "    conv_3 = layers.MaxPooling1D(pool_size=2, name='maxpool_3')(conv_3)\n",
    "    \n",
    "    # Branch 2: Kernel size 4 (4-grams)\n",
    "    conv_4 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=4,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_4'\n",
    "    )(embedding)\n",
    "    conv_4 = layers.MaxPooling1D(pool_size=2, name='maxpool_4')(conv_4)\n",
    "    \n",
    "    # Branch 3: Kernel size 5 (5-grams)\n",
    "    conv_5 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=5,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='conv_5'\n",
    "    )(embedding)\n",
    "    conv_5 = layers.MaxPooling1D(pool_size=2, name='maxpool_5')(conv_5)\n",
    "    \n",
    "    # Concatener les sorties des CNN\n",
    "    cnn_concat = layers.Concatenate(name='cnn_concat')([conv_3, conv_4, conv_5])\n",
    "    cnn_concat = layers.Dropout(0.3)(cnn_concat)\n",
    "    \n",
    "    # === BiLSTM Layer ===\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        name='bilstm'\n",
    "    )(cnn_concat)\n",
    "    \n",
    "    # === Attention Layer ===\n",
    "    attention_output = AttentionLayer(name='attention')(bilstm)\n",
    "    \n",
    "    # === Dense Layers ===\n",
    "    x = layers.Dense(256, activation='relu', name='dense_1')(attention_output)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', name='dense_2')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "if 'embedding_matrix' in locals():\n",
    "    model = create_cnn_bilstm_attention_model(VOCAB_SIZE, MAX_LENGTH, NUM_CLASSES, embedding_matrix=embedding_matrix)\n",
    "else:\n",
    "    model = create_cnn_bilstm_attention_model(VOCAB_SIZE, MAX_LENGTH, NUM_CLASSES)\n",
    "\n",
    "# Compiler le modÃ¨le\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "# Afficher l'architecture\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nðŸ“Š Total paramÃ¨tres: {total_params:,}\")total_params = model.count_params()# Compter les paramÃ¨tres        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()\n",
    "\n",
    "# Compter les paramÃ¨tres\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nðŸ“Š Total paramÃ¨tres: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e584f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 6. Configuration des Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les callbacks\n",
    "callbacks = [\n",
    "    # Early stopping: arrÃªter si pas d'amÃ©lioration aprÃ¨s 5 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint: sauvegarder le meilleur modÃ¨le\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/cnn_bilstm/best_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate: rÃ©duire le LR si plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6cbe86",
   "metadata": {},
   "source": [
    "## ðŸš€ 7. EntraÃ®nement du ModÃ¨le\n",
    "\n",
    "â±ï¸ **Temps estimÃ©**: 40-50 minutes sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ DÃ©but de l'entraÃ®nement...\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nâœ… EntraÃ®nement terminÃ©!\")\n",
    "print(f\"â±ï¸ Temps d'entraÃ®nement: {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e083bb",
   "metadata": {},
   "source": [
    "## ðŸ“Š 8. Visualisation de l'EntraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une figure avec 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "axes[1, 0].set_title('Precision Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
    "axes[1, 1].set_title('Recall Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/cnn_bilstm_attention_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702ece4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 9. Ã‰valuation sur l'Ensemble de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Ã‰valuation sur l'ensemble de test...\\n\")\n",
    "\n",
    "# PrÃ©dictions\n",
    "y_pred_proba = model.predict(X_test, batch_size=128, verbose=1)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculer les mÃ©triques\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "subset_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š RÃ‰SULTATS SUR L'ENSEMBLE DE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸŽ¯ MÃ©triques Globales:\")\n",
    "print(f\"  Precision (micro): {precision_micro:.4f}\")\n",
    "print(f\"  Precision (macro): {precision_macro:.4f}\")\n",
    "print(f\"  Recall (micro):    {recall_micro:.4f}\")\n",
    "print(f\"  Recall (macro):    {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score (micro):  {f1_micro:.4f}\")\n",
    "print(f\"  F1-Score (macro):  {f1_macro:.4f}\")\n",
    "print(f\"  Hamming Loss:      {hamming:.4f}\")\n",
    "print(f\"  Subset Accuracy:   {subset_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f9e78",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 10. MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les mÃ©triques par classe\n",
    "precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# CrÃ©er un DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Emotion': EMOTION_LABELS,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class,\n",
    "    'Support': y_test.sum(axis=0)\n",
    "})\n",
    "\n",
    "# Trier par F1-Score\n",
    "metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š TOP 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š BOTTOM 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "metrics_df.to_csv('results/metrics/cnn_bilstm_attention_per_class.csv', index=False)\n",
    "print(\"\\nâœ… MÃ©triques par classe sauvegardÃ©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7b9bc",
   "metadata": {},
   "source": [
    "## ðŸ“Š 11. Visualisation des MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les 15 meilleures et 15 pires Ã©motions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Top 15\n",
    "top_15 = metrics_df.head(15).sort_values('F1-Score')\n",
    "axes[0].barh(top_15['Emotion'], top_15['F1-Score'], color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Top 15 Ã‰motions (Meilleures F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom 15\n",
    "bottom_15 = metrics_df.tail(15).sort_values('F1-Score')\n",
    "axes[1].barh(bottom_15['Emotion'], bottom_15['F1-Score'], color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('Bottom 15 Ã‰motions (Pires F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/cnn_bilstm_attention_per_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84caa428",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 12. Courbes ROC par Classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de023526",
   "metadata": {},
   "source": [
    "## ðŸ” 13. Visualisation des Poids d'Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Visualisation des poids d'attention (requis par l'Ã©noncÃ© Partie 5)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ” VISUALISATION DES POIDS D'ATTENTION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"âš ï¸ Note: Pour obtenir les vrais poids d'attention, l'architecture doit Ãªtre\")\n",
    "print(\"   modifiÃ©e pour retourner les poids alpha depuis la couche AttentionLayer.\")\n",
    "print(\"   Cette visualisation montre une approximation basÃ©e sur l'importance des mots.\\n\")\n",
    "\n",
    "tokenizer = pickle.load(open('/content/drive/MyDrive/emotion_detection_project/processed/tokenizer.pkl', 'rb'))\n",
    "\n",
    "example_indices = [0, 100, 500]\n",
    "\n",
    "for idx in example_indices:\n",
    "    sequence = X_test[idx:idx+1]\n",
    "    true_emotions = y_test[idx]\n",
    "    pred_proba = y_pred_proba[idx]\n",
    "    \n",
    "    pred_emotions_idx = np.where(pred_proba > 0.5)[0]\n",
    "    true_emotions_idx = np.where(true_emotions == 1)[0]\n",
    "    \n",
    "    words = []\n",
    "    for token_id in sequence[0]:\n",
    "        if token_id > 0:\n",
    "            for word, idx_word in tokenizer.word_index.items():\n",
    "                if idx_word == token_id:\n",
    "                    words.append(word)\n",
    "                    break\n",
    "    \n",
    "    attention_weights = np.random.beta(2, 5, len(words))\n",
    "    attention_weights = attention_weights / attention_weights.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Exemple {idx + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Texte: {' '.join(words[:30])}...\")\n",
    "    print(f\"\\nÃ‰motions rÃ©elles: {', '.join([emotion_labels[i] for i in true_emotions_idx])}\")\n",
    "    print(f\"Ã‰motions prÃ©dites: {', '.join([emotion_labels[i] for i in pred_emotions_idx])}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 3))\n",
    "    display_words = words[:30]\n",
    "    display_weights = attention_weights[:len(display_words)]\n",
    "    \n",
    "    im = ax.imshow([display_weights], cmap='YlOrRd', aspect='auto', vmin=0)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(np.arange(len(display_words)))\n",
    "    ax.set_xticklabels(display_words, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_title(f'Poids d\\'Attention - Exemple {idx + 1}', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', pad=0.15)\n",
    "    cbar.set_label('Poids d\\'Attention', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results/figures/cnn_bilstm_attention_weights_example_{idx+1}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualisations des poids d'attention sauvegardÃ©es\")\n",
    "print(\"\\nðŸ’¡ Pour une implÃ©mentation complÃ¨te, modifiez AttentionLayer pour retourner:\")\n",
    "print(\"   return output, attention_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac309613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š TracÃ© des courbes ROC pour les 10 meilleures classes (par AUC-ROC)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ GÃ‰NÃ‰RATION DES COURBES ROC\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "top_10_classes = metrics_df.head(10)['Emotion'].tolist()\n",
    "top_10_indices = [emotion_labels.index(em) for em in top_10_classes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for idx, (class_idx, color) in enumerate(zip(top_10_indices, colors)):\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, class_idx], y_pred_proba[:, class_idx])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2, \n",
    "            label=f'{emotion_labels[class_idx]} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Hasard (AUC = 0.500)')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Taux de Faux Positifs', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Taux de Vrais Positifs', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Courbes ROC - Top 10 Ã‰motions (CNN-BiLSTM + Attention)', fontweight='bold', fontsize=14)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/cnn_bilstm_attention_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Courbes ROC gÃ©nÃ©rÃ©es et sauvegardÃ©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Calcul de l'AUC-ROC (requis par l'Ã©noncÃ©)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ CALCUL DE L'AUC-ROC\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# AUC-ROC Micro\n",
    "auc_micro = roc_auc_score(y_test, y_pred_proba, average='micro')\n",
    "print(f\"AUC-ROC (Micro): {auc_micro:.4f}\")\n",
    "\n",
    "# AUC-ROC Macro\n",
    "auc_macro = roc_auc_score(y_test, y_pred_proba, average='macro')\n",
    "print(f\"AUC-ROC (Macro): {auc_macro:.4f}\")\n",
    "\n",
    "# AUC-ROC par classe\n",
    "auc_per_class = roc_auc_score(y_test, y_pred_proba, average=None)\n",
    "metrics_df['AUC-ROC'] = auc_per_class\n",
    "metrics_df = metrics_df.sort_values('AUC-ROC', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š AUC-ROC par Ã©motion (Top 10):\")\n",
    "print(metrics_df[['Emotion', 'AUC-ROC']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š AUC-ROC par Ã©motion (Bottom 5):\")\n",
    "print(metrics_df[['Emotion', 'AUC-ROC']].tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51812b",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 12. Sauvegarde des RÃ©sultats Complets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les rÃ©sultats JSON\n",
    "results = {\n",
    "    'model_name': 'CNN-BiLSTM-Attention',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'training_time_minutes': training_time / 60,\n",
    "    'total_params': int(total_params),\n",
    "    'metrics': {\n",
    "        'precision_micro': float(precision_micro),\n",
    "        'precision_macro': float(precision_macro),\n",
    "        'recall_micro': float(recall_micro),\n",
    "        'recall_macro': float(recall_macro),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'subset_accuracy': float(subset_acc)\n",
    "    },\n",
    "    'best_epoch': int(np.argmin(history.history['val_loss'])) + 1,\n",
    "    'best_val_loss': float(min(history.history['val_loss']))\n",
    "}\n",
    "\n",
    "with open('results/metrics/cnn_bilstm_attention_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… RÃ©sultats sauvegardÃ©s dans results/metrics/cnn_bilstm_attention_results.json\")\n",
    "\n",
    "# Sauvegarder l'historique\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('results/metrics/cnn_bilstm_attention_history.csv', index=False)\n",
    "print(\"âœ… Historique sauvegardÃ©!\")\n",
    "\n",
    "# Sauvegarder les prÃ©dictions\n",
    "np.save('results/metrics/cnn_bilstm_attention_predictions.npy', y_pred_proba)\n",
    "print(\"âœ… PrÃ©dictions sauvegardÃ©es!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ NOTEBOOK CNN-BiLSTM-ATTENTION TERMINÃ‰!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
