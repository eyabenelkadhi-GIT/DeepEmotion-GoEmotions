{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f09226",
   "metadata": {},
   "source": [
    "## üì¶ 1. Installation et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6937ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "!pip install -q datasets transformers\n",
    "\n",
    "# T√©l√©charger NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f09dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7cea5",
   "metadata": {},
   "source": [
    "## üìÇ 2. Cr√©er la Structure de Dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les dossiers n√©cessaires\n",
    "folders = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'models/lstm',\n",
    "    'models/bilstm',\n",
    "    'models/cnn_bilstm',\n",
    "    'models/bert',\n",
    "    'results/figures',\n",
    "    'results/metrics'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Structure de dossiers cr√©√©e!\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eaed5",
   "metadata": {},
   "source": [
    "## üîó 3. T√©l√©chargement du Dataset GoEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger GoEmotions depuis Hugging Face\n",
    "print(\"T√©l√©chargement du dataset GoEmotions...\")\n",
    "print(\"Cela peut prendre quelques minutes...\\n\")\n",
    "\n",
    "dataset = load_dataset('go_emotions', 'simplified')\n",
    "\n",
    "# Convertir en DataFrames\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset t√©l√©charg√©!\")\n",
    "print(f\"\\nüìä Statistiques:\")\n",
    "print(f\"  Train:      {len(train_df):,} samples\")\n",
    "print(f\"  Validation: {len(val_df):,} samples\")\n",
    "print(f\"  Test:       {len(test_df):,} samples\")\n",
    "print(f\"  Total:      {len(train_df) + len(val_df) + len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset depuis data/raw/\n",
    "df = pd.read_csv('data/raw/goemotions.csv')\n",
    "\n",
    "print(\"üìã Exploration du dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"\\nColonnes: {df.columns.tolist()}\")\n",
    "print(f\"\\nPremi√®res lignes:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# V√©rifier la colonne d'√©motions\n",
    "if 'emotions' in df.columns:\n",
    "    print(f\"\\nüìä Exemples d'√©motions:\")\n",
    "    print(df['emotions'].value_counts().head(10))\n",
    "elif any(col in df.columns for col in ['anger', 'joy', 'sadness']):\n",
    "    print(\"\\n‚úÖ Dataset d√©j√† au format multi-colonnes!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è V√©rifier le format du dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les colonnes pour l'entra√Ænement\n",
    "# Le dataset Kaggle peut avoir diff√©rents formats, on s'adapte\n",
    "\n",
    "# Identifier la colonne de texte\n",
    "text_col = None\n",
    "for col in ['text', 'comment_text', 'sentence']:\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    print(\"‚ö†Ô∏è Colonne de texte non trouv√©e. Colonnes disponibles:\", df.columns.tolist())\n",
    "else:\n",
    "    print(f\"‚úÖ Colonne de texte: '{text_col}'\")\n",
    "\n",
    "# Identifier les colonnes d'√©motions\n",
    "emotion_cols = [col for col in df.columns if col in [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]]\n",
    "\n",
    "print(f\"\\nüé≠ Colonnes d'√©motions trouv√©es: {len(emotion_cols)}\")\n",
    "print(emotion_cols[:10] if len(emotion_cols) > 10 else emotion_cols)\n",
    "\n",
    "# Si le dataset a d√©j√† les 28 colonnes binaires, c'est parfait!\n",
    "if len(emotion_cols) >= 27:\n",
    "    print(\"\\n‚úÖ Dataset au format multi-label! Pr√™t pour l'entra√Ænement.\")\n",
    "    # Cr√©er les DataFrames train/val/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split: 80% train, 10% val, 10% test\n",
    "    train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"\\nüìä Splits:\")\n",
    "    print(f\"  Train:      {len(train_data):,} samples\")\n",
    "    print(f\"  Validation: {len(val_data):,} samples\")\n",
    "    print(f\"  Test:       {len(test_data):,} samples\")\n",
    "    \n",
    "    # Renommer en train_df, val_df, test_df\n",
    "    train_df = train_data.copy()\n",
    "    val_df = val_data.copy()\n",
    "    test_df = test_data.copy()\n",
    "    \n",
    "    # Cr√©er la colonne 'text' si n√©cessaire\n",
    "    if text_col != 'text':\n",
    "        train_df['text'] = train_df[text_col]\n",
    "        val_df['text'] = val_df[text_col]\n",
    "        test_df['text'] = test_df[text_col]\n",
    "    \n",
    "    # Cr√©er la colonne 'labels' (liste des indices des √©motions actives)\n",
    "    def get_label_indices(row):\n",
    "        return [i for i, col in enumerate(emotion_cols) if row[col] == 1]\n",
    "    \n",
    "    train_df['labels'] = train_df.apply(get_label_indices, axis=1)\n",
    "    val_df['labels'] = val_df.apply(get_label_indices, axis=1)\n",
    "    test_df['labels'] = test_df.apply(get_label_indices, axis=1)\n",
    "    \n",
    "    print(\"\\n‚úÖ Format adapt√© pour l'entra√Ænement!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Format non standard. Utilisation de HuggingFace comme alternative...\")\n",
    "    # Fallback sur HuggingFace si le format Kaggle est diff√©rent\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset('go_emotions', 'simplified')\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    print(f\"\\n‚úÖ Dataset HuggingFace charg√©!\")\n",
    "    print(f\"  Train:      {len(train_df):,} samples\")\n",
    "    print(f\"  Validation: {len(val_df):,} samples\")\n",
    "    print(f\"  Test:       {len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb631e",
   "metadata": {},
   "source": [
    "### üìä Pr√©paration du Dataset pour le Projet\n",
    "\n",
    "Le dataset Kaggle GoEmotions contient toutes les √©motions dans une seule colonne.\n",
    "Nous devons le transformer en format multi-label (28 colonnes binaires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un aper√ßu\n",
    "print(\"\\nüìù Aper√ßu des donn√©es:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae210b8",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 4. Labels des √âmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47032c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels d'√©motions GoEmotions\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(emotion_labels)\n",
    "\n",
    "print(f\"\\nüé≠ Nombre d'√©motions: {NUM_CLASSES}\")\n",
    "print(f\"\\nListe des √©motions:\")\n",
    "for i, emotion in enumerate(emotion_labels, 1):\n",
    "    print(f\"  {i:2d}. {emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21fa6a",
   "metadata": {},
   "source": [
    "## üìä 5. Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des √©motions\n",
    "all_labels = []\n",
    "for labels in train_df['labels']:\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "label_counts_sorted = sorted(label_counts.items())\n",
    "\n",
    "# Pr√©parer les donn√©es pour le graphique\n",
    "indices = [x[0] for x in label_counts_sorted]\n",
    "counts = [x[1] for x in label_counts_sorted]\n",
    "labels_names = [emotion_labels[i] if i < len(emotion_labels) else f\"Label {i}\" for i in indices]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "bars = ax.bar(labels_names, counts, color='steelblue', edgecolor='navy')\n",
    "\n",
    "# Colorer les barres selon la fr√©quence\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(counts)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax.set_xlabel('√âmotion', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution des √âmotions dans le Training Set', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/emotion_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Statistiques:\")\n",
    "print(f\"  Total de labels: {sum(counts):,}\")\n",
    "print(f\"  √âmotion la plus fr√©quente: {labels_names[counts.index(max(counts))]} ({max(counts):,} occurrences)\")\n",
    "print(f\"  √âmotion la moins fr√©quente: {labels_names[counts.index(min(counts))]} ({min(counts):,} occurrences)\")\n",
    "print(f\"  Ratio d√©s√©quilibre: {max(counts)/min(counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c65e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de labels par √©chantillon\n",
    "labels_per_sample = [len(labels) for labels in train_df['labels']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme\n",
    "axes[0].hist(labels_per_sample, bins=range(1, max(labels_per_sample)+2), \n",
    "             edgecolor='black', color='coral', alpha=0.7)\n",
    "axes[0].set_xlabel('Nombre de Labels par √âchantillon', fontweight='bold')\n",
    "axes[0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0].set_title('Distribution du Nombre de Labels', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(labels_per_sample, vert=True)\n",
    "axes[1].set_ylabel('Nombre de Labels', fontweight='bold')\n",
    "axes[1].set_title('Boxplot du Nombre de Labels', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/labels_per_sample.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques Multi-Label:\")\n",
    "print(f\"  Moyenne: {np.mean(labels_per_sample):.2f} labels/√©chantillon\")\n",
    "print(f\"  M√©diane: {np.median(labels_per_sample):.0f} labels/√©chantillon\")\n",
    "print(f\"  Maximum: {max(labels_per_sample)} labels/√©chantillon\")\n",
    "print(f\"  Minimum: {min(labels_per_sample)} labels/√©chantillon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cca51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur des textes\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(train_df['text_length'], bins=50, edgecolor='black', color='lightblue')\n",
    "axes[0].axvline(train_df['text_length'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Moyenne: {train_df[\"text_length\"].mean():.0f}')\n",
    "axes[0].set_xlabel('Longueur du Texte (caract√®res)', fontweight='bold')\n",
    "axes[0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0].set_title('Distribution de la Longueur des Textes', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(train_df['word_count'], bins=50, edgecolor='black', color='lightgreen')\n",
    "axes[1].axvline(train_df['word_count'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Moyenne: {train_df[\"word_count\"].mean():.1f}')\n",
    "axes[1].set_xlabel('Nombre de Mots', fontweight='bold')\n",
    "axes[1].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[1].set_title('Distribution du Nombre de Mots', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìè Statistiques de Longueur:\")\n",
    "print(f\"  Longueur moyenne: {train_df['text_length'].mean():.1f} caract√®res\")\n",
    "print(f\"  Nombre de mots moyen: {train_df['word_count'].mean():.1f} mots\")\n",
    "print(f\"  Percentile 95: {train_df['word_count'].quantile(0.95):.0f} mots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa80a06",
   "metadata": {},
   "source": [
    "## üßπ 6. Pr√©traitement des Textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Nettoyer le texte\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Supprimer mentions et hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Garder seulement lettres, chiffres et ponctuation basique\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\-]', '', text)\n",
    "    \n",
    "    # Supprimer espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "print(\"Nettoyage des textes...\")\n",
    "train_df['text_clean'] = train_df['text'].apply(clean_text)\n",
    "val_df['text_clean'] = val_df['text'].apply(clean_text)\n",
    "test_df['text_clean'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\n‚úÖ Nettoyage termin√©!\")\n",
    "print(\"\\nExemple de transformation:\")\n",
    "print(f\"Avant: {train_df['text'].iloc[0]}\")\n",
    "print(f\"Apr√®s: {train_df['text_clean'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3a8a0",
   "metadata": {},
   "source": [
    "## üî¢ 7. Tokenization et S√©quences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Taille du vocabulaire: {MAX_VOCAB_SIZE:,}\")\n",
    "print(f\"  Longueur max des s√©quences: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Cr√©er le tokenizer\n",
    "print(\"\\nCr√©ation du tokenizer...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['text_clean'])\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenizer cr√©√©!\")\n",
    "print(f\"  Vocabulaire complet: {len(tokenizer.word_index):,} mots\")\n",
    "print(f\"  Vocabulaire utilis√©: {MAX_VOCAB_SIZE:,} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en s√©quences\n",
    "print(\"Conversion en s√©quences...\")\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['text_clean'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df['text_clean'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text_clean'])\n",
    "\n",
    "# Padding\n",
    "print(\"Application du padding...\")\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                        padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                      padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                       padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\n‚úÖ S√©quences cr√©√©es!\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2b127",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 8. Pr√©parer les Labels Multi-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(labels_list, num_classes=28):\n",
    "    \"\"\"\n",
    "    Convertir les listes de labels en matrices binaires\n",
    "    \"\"\"\n",
    "    n_samples = len(labels_list)\n",
    "    label_matrix = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "    \n",
    "    for i, labels in enumerate(labels_list):\n",
    "        for label_idx in labels:\n",
    "            if label_idx < num_classes:\n",
    "                label_matrix[i, label_idx] = 1.0\n",
    "    \n",
    "    return label_matrix\n",
    "\n",
    "# Pr√©parer les labels\n",
    "print(\"Pr√©paration des labels...\")\n",
    "y_train = prepare_labels(train_df['labels'].tolist(), NUM_CLASSES)\n",
    "y_val = prepare_labels(val_df['labels'].tolist(), NUM_CLASSES)\n",
    "y_test = prepare_labels(test_df['labels'].tolist(), NUM_CLASSES)\n",
    "\n",
    "print(f\"\\n‚úÖ Labels pr√©par√©s!\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val:   {y_val.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535af99",
   "metadata": {},
   "source": [
    "## üíæ 9. Sauvegarder les Donn√©es Pr√©par√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sauvegarde des donn√©es pr√©par√©es...\\n\")\n",
    "\n",
    "# Sauvegarder les s√©quences\n",
    "np.save('data/processed/X_train.npy', X_train)\n",
    "np.save('data/processed/X_val.npy', X_val)\n",
    "np.save('data/processed/X_test.npy', X_test)\n",
    "print(\"‚úÖ S√©quences sauvegard√©es\")\n",
    "\n",
    "# Sauvegarder les labels\n",
    "np.save('data/processed/y_train.npy', y_train)\n",
    "np.save('data/processed/y_val.npy', y_val)\n",
    "np.save('data/processed/y_test.npy', y_test)\n",
    "print(\"‚úÖ Labels sauvegard√©s\")\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "with open('data/processed/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"‚úÖ Tokenizer sauvegard√©\")\n",
    "\n",
    "# Sauvegarder les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'emotion_labels': emotion_labels,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'vocab_size': len(tokenizer.word_index),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test)\n",
    "}\n",
    "\n",
    "with open('data/processed/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"‚úÖ M√©tadonn√©es sauvegard√©es\")\n",
    "\n",
    "# Sauvegarder les textes originaux pour analyse\n",
    "with open('data/processed/test_texts.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df['text'].tolist(), f)\n",
    "print(\"‚úÖ Textes de test sauvegard√©s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PR√âPARATION DES DONN√âES TERMIN√âE !\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVous pouvez maintenant ex√©cuter les notebooks d'entra√Ænement:\")\n",
    "print(\"  üìì Notebook_1_LSTM.ipynb\")\n",
    "print(\"  üìì Notebook_2_BiLSTM_Attention.ipynb\")\n",
    "print(\"  üìì Notebook_3_CNN_BiLSTM.ipynb\")\n",
    "print(\"  üìì Notebook_4_BERT.ipynb\")\n",
    "print(\"\\nPuis pour la comparaison:\")\n",
    "print(\"  üìì Notebook_5_Comparaison_Finale.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les fichiers cr√©√©s\n",
    "print(\"\\nüìÅ Fichiers cr√©√©s dans data/processed/:\")\n",
    "!ls -lh data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4701f",
   "metadata": {},
   "source": [
    "## üì§ 10. [Optionnel] Sauvegarder vers Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive (optionnel)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er un dossier dans Drive\n",
    "!mkdir -p \"/content/drive/MyDrive/emotion_detection_project\"\n",
    "\n",
    "# Copier les donn√©es\n",
    "!cp -r data/processed \"/content/drive/MyDrive/emotion_detection_project/\"\n",
    "!cp -r results \"/content/drive/MyDrive/emotion_detection_project/\"\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es sauvegard√©es dans Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
