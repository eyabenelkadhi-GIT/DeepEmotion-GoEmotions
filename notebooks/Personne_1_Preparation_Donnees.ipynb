{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f09226",
   "metadata": {},
   "source": [
    "## üì¶ 1. Installation et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6937ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "!pip install -q datasets transformers scikit-learn kagglehub\n",
    "\n",
    "# T√©l√©charger NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f09dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7cea5",
   "metadata": {},
   "source": [
    "## üìÇ 2. Cr√©er la Structure de Dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les dossiers n√©cessaires\n",
    "folders = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'models/lstm',\n",
    "    'models/bilstm',\n",
    "    'models/cnn_bilstm',\n",
    "    'models/bert',\n",
    "    'results/figures',\n",
    "    'results/metrics'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Structure de dossiers cr√©√©e!\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eaed5",
   "metadata": {},
   "source": [
    "## üîó 3. T√©l√©chargement du Dataset GoEmotions depuis Kaggle\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT : Personne 1 doit faire cette √©tape**\n",
    "\n",
    "**Source** : https://www.kaggle.com/datasets/debarshichanda/goemotions/data\n",
    "\n",
    "### M√©thode 1 : Chargement Automatique avec KaggleHub (Recommand√© ‚úÖ)\n",
    "Utilise l'API Kaggle pour t√©l√©charger automatiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# M√âTHODE 1 : Chargement Automatique (Recommand√©)\n",
    "# ==========================================\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"üì• T√©l√©chargement automatique du dataset GoEmotions depuis Kaggle...\")\n",
    "print(\"‚è≥ Cela peut prendre 1-2 minutes...\\n\")\n",
    "\n",
    "try:\n",
    "    # T√©l√©charger le dataset (nouvelle API)\n",
    "    path = kagglehub.dataset_download(\"debarshichanda/goemotions\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset t√©l√©charg√© dans: {path}\")\n",
    "    \n",
    "    # Lister les fichiers disponibles\n",
    "    files_in_dataset = os.listdir(path)\n",
    "    print(f\"\\nüìÅ Fichiers disponibles: {files_in_dataset}\")\n",
    "    \n",
    "    # Trouver le fichier CSV principal\n",
    "    csv_files = [f for f in files_in_dataset if f.endswith('.csv')]\n",
    "    \n",
    "    if csv_files:\n",
    "        # Charger le premier fichier CSV trouv√©\n",
    "        csv_file = csv_files[0]\n",
    "        file_path = os.path.join(path, csv_file)\n",
    "        \n",
    "        print(f\"\\nüìä Chargement du fichier: {csv_file}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Dataset charg√© avec succ√®s!\")\n",
    "        print(f\"üìä Statistiques:\")\n",
    "        print(f\"  Nombre total d'√©chantillons: {len(df):,}\")\n",
    "        print(f\"  Nombre de colonnes: {len(df.columns)}\")\n",
    "        \n",
    "        # Sauvegarder dans data/raw/\n",
    "        df.to_csv('data/raw/goemotions.csv', index=False)\n",
    "        print(\"\\nüíæ Dataset sauvegard√© dans data/raw/goemotions.csv\")\n",
    "        \n",
    "        print(f\"\\nüìã Colonnes disponibles:\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        print(f\"\\nüëÅÔ∏è Aper√ßu des 5 premi√®res lignes:\")\n",
    "        display(df.head())\n",
    "    else:\n",
    "        print(\"‚ùå Aucun fichier CSV trouv√© dans le dataset\")\n",
    "        print(\"üí° Utilisez la M√©thode 2 (Upload Manuel) ci-dessous\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur lors du chargement automatique: {e}\")\n",
    "    print(\"\\nüí° Solution : Utilisez la M√©thode 2 (Upload Manuel) ci-dessous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35113b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# M√âTHODE 2 : Upload Manuel (Alternative)\n",
    "# ==========================================\n",
    "# ‚ö†Ô∏è Ex√©cutez cette cellule si la M√©thode 1 a √©chou√©\n",
    "\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üì§ Veuillez uploader les fichiers CSV du dataset GoEmotions depuis Kaggle...\")\n",
    "print(\"üëâ T√©l√©chargez d'abord depuis: https://www.kaggle.com/datasets/debarshichanda/goemotions/data\")\n",
    "print(\"üëâ Le dataset peut √™tre en 1 ou 3 fichiers (goemotions.csv OU goemotions_1/2/3.csv)\")\n",
    "print(\"üëâ Uploadez TOUS les fichiers CSV, puis cliquez sur 'Choisir les fichiers' ci-dessous\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Trouver tous les fichiers CSV upload√©s\n",
    "csv_files = [name for name in uploaded.keys() if name.endswith('.csv')]\n",
    "\n",
    "if csv_files:\n",
    "    print(f\"\\n‚úÖ {len(csv_files)} fichier(s) CSV d√©tect√©(s): {csv_files}\")\n",
    "    \n",
    "    # Charger tous les fichiers CSV et les combiner\n",
    "    print(\"üìä Chargement des donn√©es en cours...\")\n",
    "    dfs = []\n",
    "    for csv_file in sorted(csv_files):  # Trier pour avoir 1, 2, 3 dans l'ordre\n",
    "        print(f\"  - Chargement de {csv_file}...\")\n",
    "        df_temp = pd.read_csv(csv_file)\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"    ‚úì {len(df_temp):,} lignes charg√©es\")\n",
    "    \n",
    "    # Combiner tous les DataFrames\n",
    "    if len(dfs) > 1:\n",
    "        print(f\"\\nüîó Combinaison de {len(dfs)} fichiers...\")\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Fichiers combin√©s!\")\n",
    "    else:\n",
    "        df = dfs[0]\n",
    "    \n",
    "    # Sauvegarder dans data/raw/\n",
    "    df.to_csv('data/raw/goemotions.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset charg√© avec succ√®s!\")\n",
    "    print(f\"üìä Statistiques:\")\n",
    "    print(f\"  Nombre total d'√©chantillons: {len(df):,}\")\n",
    "    print(f\"  Nombre de colonnes: {len(df.columns)}\")\n",
    "    print(f\"\\nüìã Colonnes disponibles:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(f\"\\nüëÅÔ∏è Aper√ßu des premi√®res lignes:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"‚ùå ERREUR : Aucun fichier CSV upload√©!\")\n",
    "    print(\"üëâ T√©l√©chargez le dataset depuis: https://www.kaggle.com/datasets/debarshichanda/goemotions/data\")\n",
    "    print(\"üëâ Puis r√©ex√©cutez cette cellule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a28a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# M√âTHODE 3 : Charger depuis les fichiers d√©j√† upload√©s\n",
    "# ==========================================\n",
    "# ‚ö†Ô∏è Si vous avez d√©j√† upload√© les fichiers et ils sont dans /content/\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Liste des fichiers CSV √† charger (modifiez les chemins si n√©cessaire)\n",
    "csv_files = [\n",
    "    '/content/goemotions_1.csv',\n",
    "    '/content/goemotions_2.csv',\n",
    "    '/content/goemotions_3.csv'\n",
    "]\n",
    "\n",
    "# V√©rifier quels fichiers existent\n",
    "existing_files = [f for f in csv_files if os.path.exists(f)]\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"‚úÖ {len(existing_files)} fichier(s) trouv√©(s)!\")\n",
    "    \n",
    "    # Charger tous les fichiers\n",
    "    dfs = []\n",
    "    for csv_file in existing_files:\n",
    "        print(f\"üìä Chargement de {csv_file}...\")\n",
    "        df_temp = pd.read_csv(csv_file)\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"  ‚úì {len(df_temp):,} lignes charg√©es\")\n",
    "    \n",
    "    # Combiner tous les DataFrames\n",
    "    if len(dfs) > 1:\n",
    "        print(f\"\\nüîó Combinaison de {len(dfs)} fichiers...\")\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Fichiers combin√©s!\")\n",
    "    else:\n",
    "        df = dfs[0]\n",
    "    \n",
    "    # Sauvegarder dans data/raw/\n",
    "    df.to_csv('data/raw/goemotions.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset charg√© avec succ√®s!\")\n",
    "    print(f\"üìä Statistiques:\")\n",
    "    print(f\"  Nombre total d'√©chantillons: {len(df):,}\")\n",
    "    print(f\"  Nombre de colonnes: {len(df.columns)}\")\n",
    "    print(f\"\\nüìã Colonnes disponibles:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(f\"\\nüëÅÔ∏è Aper√ßu des premi√®res lignes:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"‚ùå Aucun fichier trouv√©!\")\n",
    "    print(\"üí° V√©rifiez les chemins des fichiers. Ex√©cutez cette commande pour voir o√π sont vos fichiers:\")\n",
    "    print(\"!ls -la /content/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8d480",
   "metadata": {},
   "source": [
    "### M√©thode 2 : Upload Manuel (Si la M√©thode 1 √©choue)\n",
    "\n",
    "**Instructions :**\n",
    "1. Aller sur https://www.kaggle.com/datasets/debarshichanda/goemotions/data\n",
    "2. Cliquer sur **\"Download\"** pour t√©l√©charger goemotions.csv sur votre PC\n",
    "3. Ex√©cuter la cellule ci-dessous et cliquer sur **\"Choisir les fichiers\"**\n",
    "4. S√©lectionner le fichier goemotions.csv t√©l√©charg√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# √âTAPE 2 : V√©rification du format du dataset\n",
    "# ==========================================\n",
    "\n",
    "print(\"üîç Analyse du format du dataset...\\n\")\n",
    "\n",
    "# V√©rifier les colonnes\n",
    "print(f\"üìä Informations du dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Colonnes: {list(df.columns)}\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(f\"\\nüìù Exemples de donn√©es:\")\n",
    "display(df.head(10))\n",
    "\n",
    "# V√©rifier les statistiques\n",
    "print(f\"\\nüìà Statistiques de base:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# √âTAPE 3 : Pr√©paration du dataset pour l'entra√Ænement\n",
    "# ==========================================\n",
    "\n",
    "print(\"üîß Pr√©paration du dataset au format multi-label...\\n\")\n",
    "\n",
    "# Identifier la colonne de texte\n",
    "text_col = None\n",
    "for col in ['text', 'comment_text', 'sentence', 'content']:\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col is None:\n",
    "    print(\"‚ùå Colonne de texte non trouv√©e!\")\n",
    "    print(f\"Colonnes disponibles: {df.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Colonne de texte identifi√©e: '{text_col}'\")\n",
    "\n",
    "# Identifier les colonnes d'√©motions (28 √©motions GoEmotions)\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "emotion_cols = [col for col in df.columns if col in emotion_labels]\n",
    "\n",
    "print(f\"\\nüé≠ Colonnes d'√©motions trouv√©es: {len(emotion_cols)}/{len(emotion_labels)}\")\n",
    "if len(emotion_cols) > 0:\n",
    "    print(f\"√âmotions: {emotion_cols[:5]}... (affichant 5 premi√®res)\")\n",
    "\n",
    "# Cr√©er les splits train/val/test (80/10/10)\n",
    "if len(emotion_cols) >= 27 and text_col is not None:\n",
    "    print(\"\\n‚úÖ Dataset au format multi-label! Cr√©ation des splits...\")\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split: 80% train, 10% val, 10% test\n",
    "    train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, shuffle=True)\n",
    "    \n",
    "    print(f\"\\nüìä Splits cr√©√©s:\")\n",
    "    print(f\"  Train:      {len(train_data):,} samples ({len(train_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val_data):,} samples ({len(val_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Test:       {len(test_data):,} samples ({len(test_data)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Total:      {len(df):,} samples\")\n",
    "    \n",
    "    # Cr√©er les DataFrames finaux\n",
    "    train_df = train_data.copy()\n",
    "    val_df = val_data.copy()\n",
    "    test_df = test_data.copy()\n",
    "    \n",
    "    # Normaliser la colonne 'text'\n",
    "    if text_col != 'text':\n",
    "        train_df['text'] = train_df[text_col]\n",
    "        val_df['text'] = val_df[text_col]\n",
    "        test_df['text'] = test_df[text_col]\n",
    "    \n",
    "    # Cr√©er la colonne 'labels' (liste des indices des √©motions actives)\n",
    "    def get_label_indices(row):\n",
    "        return [i for i, col in enumerate(emotion_labels) if col in emotion_cols and row.get(col, 0) == 1]\n",
    "    \n",
    "    print(\"\\nüè∑Ô∏è Cr√©ation de la colonne 'labels'...\")\n",
    "    train_df['labels'] = train_df.apply(get_label_indices, axis=1)\n",
    "    val_df['labels'] = val_df.apply(get_label_indices, axis=1)\n",
    "    test_df['labels'] = test_df.apply(get_label_indices, axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Dataset pr√™t pour l'entra√Ænement!\")\n",
    "    print(f\"\\nüìã Exemple de labels:\")\n",
    "    print(f\"  Texte: {train_df.iloc[0]['text'][:80]}...\")\n",
    "    print(f\"  Labels: {train_df.iloc[0]['labels']}\")\n",
    "    print(f\"  √âmotions: {[emotion_labels[i] for i in train_df.iloc[0]['labels']]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Format du dataset non reconnu. Utilisation de HuggingFace comme fallback...\")\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset('go_emotions', 'simplified')\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    emotion_labels = [\n",
    "        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "        'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "    ]\n",
    "    print(f\"‚úÖ Dataset HuggingFace charg√©!\")\n",
    "    print(f\"  Train:      {len(train_df):,} samples\")\n",
    "    print(f\"  Validation: {len(val_df):,} samples\")\n",
    "    print(f\"  Test:       {len(test_df):,} samples\")\n",
    "\n",
    "NUM_CLASSES = len(emotion_labels)\n",
    "print(f\"\\nüéØ Nombre de classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb631e",
   "metadata": {},
   "source": [
    "### üìä Pr√©paration du Dataset pour le Projet\n",
    "\n",
    "Le dataset Kaggle GoEmotions contient toutes les √©motions dans une seule colonne.\n",
    "Nous devons le transformer en format multi-label (28 colonnes binaires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un aper√ßu\n",
    "print(\"\\nüìù Aper√ßu des donn√©es:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae210b8",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 4. Labels des √âmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47032c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels d'√©motions GoEmotions\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(emotion_labels)\n",
    "\n",
    "print(f\"\\nüé≠ Nombre d'√©motions: {NUM_CLASSES}\")\n",
    "print(f\"\\nListe des √©motions:\")\n",
    "for i, emotion in enumerate(emotion_labels, 1):\n",
    "    print(f\"  {i:2d}. {emotion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21fa6a",
   "metadata": {},
   "source": [
    "## üìä 5. Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des √©motions\n",
    "all_labels = []\n",
    "for labels in train_df['labels']:\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "label_counts_sorted = sorted(label_counts.items())\n",
    "\n",
    "# Pr√©parer les donn√©es pour le graphique\n",
    "indices = [x[0] for x in label_counts_sorted]\n",
    "counts = [x[1] for x in label_counts_sorted]\n",
    "labels_names = [emotion_labels[i] if i < len(emotion_labels) else f\"Label {i}\" for i in indices]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "bars = ax.bar(labels_names, counts, color='steelblue', edgecolor='navy')\n",
    "\n",
    "# Colorer les barres selon la fr√©quence\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(counts)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax.set_xlabel('√âmotion', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution des √âmotions dans le Training Set', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/emotion_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Statistiques:\")\n",
    "print(f\"  Total de labels: {sum(counts):,}\")\n",
    "print(f\"  √âmotion la plus fr√©quente: {labels_names[counts.index(max(counts))]} ({max(counts):,} occurrences)\")\n",
    "print(f\"  √âmotion la moins fr√©quente: {labels_names[counts.index(min(counts))]} ({min(counts):,} occurrences)\")\n",
    "print(f\"  Ratio d√©s√©quilibre: {max(counts)/min(counts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c65e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de labels par √©chantillon\n",
    "labels_per_sample = [len(labels) for labels in train_df['labels']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme\n",
    "axes[0].hist(labels_per_sample, bins=range(1, max(labels_per_sample)+2), \n",
    "             edgecolor='black', color='coral', alpha=0.7)\n",
    "axes[0].set_xlabel('Nombre de Labels par √âchantillon', fontweight='bold')\n",
    "axes[0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0].set_title('Distribution du Nombre de Labels', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(labels_per_sample, vert=True)\n",
    "axes[1].set_ylabel('Nombre de Labels', fontweight='bold')\n",
    "axes[1].set_title('Boxplot du Nombre de Labels', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/labels_per_sample.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques Multi-Label:\")\n",
    "print(f\"  Moyenne: {np.mean(labels_per_sample):.2f} labels/√©chantillon\")\n",
    "print(f\"  M√©diane: {np.median(labels_per_sample):.0f} labels/√©chantillon\")\n",
    "print(f\"  Maximum: {max(labels_per_sample)} labels/√©chantillon\")\n",
    "print(f\"  Minimum: {min(labels_per_sample)} labels/√©chantillon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cca51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longueur des textes\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(train_df['text_length'], bins=50, edgecolor='black', color='lightblue')\n",
    "axes[0].axvline(train_df['text_length'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Moyenne: {train_df[\"text_length\"].mean():.0f}')\n",
    "axes[0].set_xlabel('Longueur du Texte (caract√®res)', fontweight='bold')\n",
    "axes[0].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[0].set_title('Distribution de la Longueur des Textes', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(train_df['word_count'], bins=50, edgecolor='black', color='lightgreen')\n",
    "axes[1].axvline(train_df['word_count'].mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Moyenne: {train_df[\"word_count\"].mean():.1f}')\n",
    "axes[1].set_xlabel('Nombre de Mots', fontweight='bold')\n",
    "axes[1].set_ylabel('Fr√©quence', fontweight='bold')\n",
    "axes[1].set_title('Distribution du Nombre de Mots', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìè Statistiques de Longueur:\")\n",
    "print(f\"  Longueur moyenne: {train_df['text_length'].mean():.1f} caract√®res\")\n",
    "print(f\"  Nombre de mots moyen: {train_df['word_count'].mean():.1f} mots\")\n",
    "print(f\"  Percentile 95: {train_df['word_count'].quantile(0.95):.0f} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0363b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä ANALYSE DE CO-OCCURRENCE DES √âMOTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cr√©er la matrice de co-occurrence\n",
    "# D'abord pr√©parer y_train si pas encore fait (pour cette section)\n",
    "if 'y_train' not in locals():\n",
    "    y_train_temp = prepare_labels(train_df['labels'].tolist(), NUM_CLASSES)\n",
    "else:\n",
    "    y_train_temp = y_train\n",
    "\n",
    "co_occurrence = np.dot(y_train_temp.T, y_train_temp)\n",
    "\n",
    "# Normaliser pour avoir des probabilit√©s\n",
    "co_occurrence_prob = co_occurrence / np.diag(co_occurrence)[:, None]\n",
    "\n",
    "# Cr√©er un DataFrame pour meilleure visualisation\n",
    "co_occurrence_df = pd.DataFrame(\n",
    "    co_occurrence_prob,\n",
    "    index=emotion_labels,\n",
    "    columns=emotion_labels\n",
    ")\n",
    "\n",
    "# Visualiser la matrice\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(\n",
    "    co_occurrence_df,\n",
    "    cmap='YlOrRd',\n",
    "    annot=False,  # Trop de donn√©es pour annoter\n",
    "    fmt='.2f',\n",
    "    cbar_kws={'label': 'Probabilit√© de co-occurrence'},\n",
    "    linewidths=0.5,\n",
    "    square=True\n",
    ")\n",
    "plt.title('Matrice de Co-occurrence des √âmotions\\n(Probabilit√© qu\\'une √©motion apparaisse avec une autre)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('√âmotion', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('√âmotion', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/emotion_cooccurrence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Trouver les paires d'√©motions les plus corr√©l√©es\n",
    "print(\"\\nüîó TOP 10 PAIRES D'√âMOTIONS LES PLUS CORR√âL√âES:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "correlations = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(i+1, NUM_CLASSES):\n",
    "        correlations.append({\n",
    "            'Emotion1': emotion_labels[i],\n",
    "            'Emotion2': emotion_labels[j],\n",
    "            'Correlation': co_occurrence_prob[i, j]\n",
    "        })\n",
    "\n",
    "correlations_df = pd.DataFrame(correlations).sort_values('Correlation', ascending=False)\n",
    "print(correlations_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Analyse de co-occurrence termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32788403",
   "metadata": {},
   "source": [
    "### üìä 5.5 Matrice de Co-occurrence des √âmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les ratios de d√©s√©quilibre\n",
    "print(\"\\nüìä ANALYSE DU D√âS√âQUILIBRE DES CLASSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_count = max(counts)\n",
    "min_count = min(counts)\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\nRatio de d√©s√©quilibre : {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Classe majoritaire : {labels_names[counts.index(max_count)]} ({max_count:,} occurrences)\")\n",
    "print(f\"Classe minoritaire : {labels_names[counts.index(min_count)]} ({min_count:,} occurrences)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculer les poids de classe pour g√©rer le d√©s√©quilibre\n",
    "emotion_weights = {}\n",
    "for emotion, count in zip(emotion_labels, counts):\n",
    "    weight = len(train_df) / (NUM_CLASSES * count)\n",
    "    emotion_weights[emotion] = weight\n",
    "\n",
    "# Cr√©er un DataFrame des poids\n",
    "weights_df = pd.DataFrame({\n",
    "    'Emotion': emotion_labels,\n",
    "    'Count': counts,\n",
    "    'Weight': [emotion_weights[em] for em in emotion_labels]\n",
    "}).sort_values('Weight', ascending=False)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è POIDS DES CLASSES (Top 10 et Bottom 10):\")\n",
    "print(f\"\\nTop 10 (classes sous-repr√©sent√©es - poids √©lev√©s):\")\n",
    "print(weights_df.head(10).to_string(index=False))\n",
    "print(f\"\\nBottom 10 (classes sur-repr√©sent√©es - poids faibles):\")\n",
    "print(weights_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Visualiser les poids\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = ax.bar(weights_df['Emotion'], weights_df['Weight'], alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Colorer selon le poids\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0, 1, len(weights_df)))\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax.set_xlabel('√âmotion', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Poids de Classe', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Poids des Classes pour G√©rer le D√©s√©quilibre', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Poids √©quilibr√© (1.0)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/class_weights.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Analyse du d√©s√©quilibre termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b5ca6",
   "metadata": {},
   "source": [
    "### üìä 5.4 Analyse Approfondie du D√©s√©quilibre des Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa80a06",
   "metadata": {},
   "source": [
    "## üßπ 6. Pr√©traitement des Textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Nettoyer le texte\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Supprimer mentions et hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Garder seulement lettres, chiffres et ponctuation basique\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\-]', '', text)\n",
    "    \n",
    "    # Supprimer espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "print(\"Nettoyage des textes...\")\n",
    "train_df['text_clean'] = train_df['text'].apply(clean_text)\n",
    "val_df['text_clean'] = val_df['text'].apply(clean_text)\n",
    "test_df['text_clean'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\n‚úÖ Nettoyage termin√©!\")\n",
    "print(\"\\nExemple de transformation:\")\n",
    "print(f\"Avant: {train_df['text'].iloc[0]}\")\n",
    "print(f\"Apr√®s: {train_df['text_clean'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3a8a0",
   "metadata": {},
   "source": [
    "## üî¢ 7. Tokenization et S√©quences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Taille du vocabulaire: {MAX_VOCAB_SIZE:,}\")\n",
    "print(f\"  Longueur max des s√©quences: {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "# Cr√©er le tokenizer\n",
    "print(\"\\nCr√©ation du tokenizer...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['text_clean'])\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenizer cr√©√©!\")\n",
    "print(f\"  Vocabulaire complet: {len(tokenizer.word_index):,} mots\")\n",
    "print(f\"  Vocabulaire utilis√©: {MAX_VOCAB_SIZE:,} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en s√©quences\n",
    "print(\"Conversion en s√©quences...\")\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['text_clean'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df['text_clean'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['text_clean'])\n",
    "\n",
    "# Padding\n",
    "print(\"Application du padding...\")\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                        padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                      padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                       padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\n‚úÖ S√©quences cr√©√©es!\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2b127",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 8. Pr√©parer les Labels Multi-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(labels_list, num_classes=28):\n",
    "    \"\"\"\n",
    "    Convertir les listes de labels en matrices binaires\n",
    "    \"\"\"\n",
    "    n_samples = len(labels_list)\n",
    "    label_matrix = np.zeros((n_samples, num_classes), dtype=np.float32)\n",
    "    \n",
    "    for i, labels in enumerate(labels_list):\n",
    "        for label_idx in labels:\n",
    "            if label_idx < num_classes:\n",
    "                label_matrix[i, label_idx] = 1.0\n",
    "    \n",
    "    return label_matrix\n",
    "\n",
    "# Pr√©parer les labels\n",
    "print(\"Pr√©paration des labels...\")\n",
    "y_train = prepare_labels(train_df['labels'].tolist(), NUM_CLASSES)\n",
    "y_val = prepare_labels(val_df['labels'].tolist(), NUM_CLASSES)\n",
    "y_test = prepare_labels(test_df['labels'].tolist(), NUM_CLASSES)\n",
    "\n",
    "print(f\"\\n‚úÖ Labels pr√©par√©s!\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val:   {y_val.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# √âTAPE 8.1 : Gestion du D√©s√©quilibre des Classes (Requis pour l'√©nonc√©)\n",
    "# ==========================================\n",
    "print(\"\\n‚öñÔ∏è Calcul des poids de classes pour g√©rer le d√©s√©quilibre...\")\n",
    "\n",
    "# Calculer le nombre d'occurrences positives pour chaque classe\n",
    "class_counts = y_train.sum(axis=0)\n",
    "total_samples = len(y_train)\n",
    "\n",
    "# Calculer les poids (plus une classe est rare, plus son poids est √©lev√©)\n",
    "# Formule standard pour multi-label: pos_weight = (total - pos) / pos\n",
    "class_weights = {}\n",
    "pos_weights = []\n",
    "\n",
    "print(\"Poids calcul√©s (Top 5 rares vs fr√©quents):\")\n",
    "sorted_indices = np.argsort(class_counts)\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    pos = max(class_counts[i], 1) # √âviter division par 0\n",
    "    neg = total_samples - pos\n",
    "    weight = neg / pos\n",
    "    class_weights[i] = weight\n",
    "    pos_weights.append(weight)\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"\\nClasses les plus rares (Poids √©lev√©s):\")\n",
    "for i in sorted_indices[:5]:\n",
    "    print(f\"  {emotion_labels[i]}: Count={int(class_counts[i])}, Weight={class_weights[i]:.2f}\")\n",
    "\n",
    "print(\"\\nClasses les plus fr√©quentes (Poids faibles):\")\n",
    "for i in sorted_indices[-5:]:\n",
    "    print(f\"  {emotion_labels[i]}: Count={int(class_counts[i])}, Weight={class_weights[i]:.2f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# √âTAPE 8.2 : Pr√©paration des Embeddings GloVe (Requis pour l'√©tude d'ablation)\n",
    "# ==========================================\n",
    "# Note: L'√©nonc√© demande de comparer les embeddings. Nous pr√©parons GloVe ici.\n",
    "print(\"\\nüåç Pr√©paration des Embeddings GloVe (Pre-trained)...\")\n",
    "\n",
    "# Fonction pour charger GloVe\n",
    "def load_glove_embeddings(vocab_word_index, embedding_dim=100):\n",
    "    print(\"  üì• T√©l√©chargement de GloVe (glove.6B.100d)... Cela peut prendre quelques instants.\")\n",
    "    \n",
    "    # V√©rifier si le fichier existe d√©j√†\n",
    "    if not os.path.exists('glove.6B.100d.txt'):\n",
    "        print(\"  Downloading GloVe...\")\n",
    "        !wget -q --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip\n",
    "        !unzip -q glove.6B.zip\n",
    "    \n",
    "    print(\"  Chargement des vecteurs en m√©moire...\")\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(f\"  ‚úÖ {len(embeddings_index):,} vecteurs de mots trouv√©s dans GloVe.\")\n",
    "\n",
    "    # Cr√©er la matrice d'embedding seulement pour notre vocabulaire\n",
    "    num_words = min(MAX_VOCAB_SIZE, len(vocab_word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for word, i in vocab_word_index.items():\n",
    "        if i >= MAX_VOCAB_SIZE:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Les mots non trouv√©s dans l'embedding index seront tous des z√©ros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            \n",
    "    print(f\"  ‚úÖ Matrice construite: {hits} mots trouv√©s, {misses} mots manquants.\")\n",
    "    return embedding_matrix\n",
    "\n",
    "# Pr√©parer la matrice au cas o√π on voudrait l'utiliser\n",
    "try:\n",
    "    embedding_matrix = load_glove_embeddings(tokenizer.word_index, embedding_dim=128) # Note: GloVe est 100d, on adaptera ou on utilisera 100d\n",
    "    # Note: Si on utilise glove.6B.100d, la dimension DOIT √™tre 100.\n",
    "    # Pour simplifier et matcher GloVe standard, on refait avec dim=100\n",
    "    embedding_matrix = load_glove_embeddings(tokenizer.word_index, embedding_dim=100)\n",
    "    USE_GLOVE = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Impossible de charger GloVe (Erreur: {e}). On continuera sans.\")\n",
    "    embedding_matrix = None\n",
    "    USE_GLOVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535af99",
   "metadata": {},
   "source": [
    "## üíæ 9. Sauvegarder les Donn√©es Pr√©par√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sauvegarde des donn√©es pr√©par√©es...\\n\")\n",
    "\n",
    "# Sauvegarder les s√©quences\n",
    "np.save('data/processed/X_train.npy', X_train)\n",
    "np.save('data/processed/X_val.npy', X_val)\n",
    "np.save('data/processed/X_test.npy', X_test)\n",
    "print(\"‚úÖ S√©quences sauvegard√©es\")\n",
    "\n",
    "# Sauvegarder les labels\n",
    "np.save('data/processed/y_train.npy', y_train)\n",
    "np.save('data/processed/y_val.npy', y_val)\n",
    "np.save('data/processed/y_test.npy', y_test)\n",
    "print(\"‚úÖ Labels sauvegard√©s\")\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "with open('data/processed/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"‚úÖ Tokenizer sauvegard√©\")\n",
    "\n",
    "# Sauvegarder les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'emotion_labels': emotion_labels,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'vocab_size': len(tokenizer.word_index),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'use_glove': USE_GLOVE\n",
    "}\n",
    "\n",
    "with open('data/processed/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"‚úÖ M√©tadonn√©es sauvegard√©es\")\n",
    "\n",
    "# Sauvegarder les poids de classes (NOUVEAU - Requis Partie 1)\n",
    "with open('data/processed/class_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(class_weights, f)\n",
    "print(\"‚úÖ Poids des classes sauvegard√©s (pour g√©rer le d√©s√©quilibre)\")\n",
    "\n",
    "# Sauvegarder la matrice d'embedding GloVe (NOUVEAU - Requis Partie 4)\n",
    "if USE_GLOVE and embedding_matrix is not None:\n",
    "    np.save('data/processed/embedding_matrix.npy', embedding_matrix)\n",
    "    print(\"‚úÖ Matrice d'embedding GloVe sauvegard√©e\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Matrice GloVe non sauvegard√©e (non charg√©e)\")\n",
    "\n",
    "# Sauvegarder les textes originaux pour BERT (qui a besoin des textes bruts)\n",
    "with open('data/processed/texts_train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_df['text_clean'].tolist(), f)\n",
    "print(\"‚úÖ Textes train sauvegard√©s\")\n",
    "\n",
    "with open('data/processed/texts_val.pkl', 'wb') as f:\n",
    "    pickle.dump(val_df['text_clean'].tolist(), f)\n",
    "print(\"‚úÖ Textes val sauvegard√©s\")\n",
    "\n",
    "with open('data/processed/texts_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df['text_clean'].tolist(), f)\n",
    "print(\"‚úÖ Textes test sauvegard√©s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PR√âPARATION DES DONN√âES TERMIN√âE !\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVous pouvez maintenant ex√©cuter les notebooks d'entra√Ænement:\")\n",
    "print(\"  üìì Notebook_1_LSTM.ipynb\")\n",
    "print(\"  üìì Notebook_2_BiLSTM_Attention.ipynb\")\n",
    "print(\"  üìì Notebook_3_CNN_BiLSTM.ipynb\")\n",
    "print(\"  üìì Notebook_4_BERT.ipynb\")\n",
    "print(\"\\nPuis pour la comparaison:\")\n",
    "print(\"  üìì Notebook_5_Comparaison_Finale.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les fichiers cr√©√©s\n",
    "print(\"\\nüìÅ Fichiers cr√©√©s dans data/processed/:\")\n",
    "!ls -lh data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4701f",
   "metadata": {},
   "source": [
    "## üì§ 10. [Optionnel] Sauvegarder vers Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive (optionnel)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er un dossier dans Drive\n",
    "!mkdir -p \"/content/drive/MyDrive/emotion_detection_project\"\n",
    "\n",
    "# Copier les donn√©es\n",
    "!cp -r data/processed \"/content/drive/MyDrive/emotion_detection_project/\"\n",
    "!cp -r results \"/content/drive/MyDrive/emotion_detection_project/\"\n",
    "\n",
    "print(\"\\n‚úÖ Donn√©es sauvegard√©es dans Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
