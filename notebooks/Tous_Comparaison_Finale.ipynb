{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c59958",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ab7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"âœ… Imports rÃ©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c8673",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 2. CrÃ©ation des Dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les dossiers si nÃ©cessaire\n",
    "folders = [\n",
    "    'results/figures',\n",
    "    'results/metrics',\n",
    "    'results/comparison'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"âœ… Dossiers crÃ©Ã©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab637c0",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 3. Chargement des RÃ©sultats de Tous les ModÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¥ Chargement des rÃ©sultats...\\n\")\n",
    "\n",
    "# Liste des modÃ¨les\n",
    "models = ['lstm', 'bilstm_attention', 'cnn_bilstm_attention', 'bert']\n",
    "model_names = ['LSTM', 'BiLSTM+Attention', 'CNN-BiLSTM+Attention', 'BERT']\n",
    "\n",
    "# Charger les rÃ©sultats\n",
    "results_dict = {}\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    try:\n",
    "        with open(f'results/metrics/{model}_results.json', 'r') as f:\n",
    "            results_dict[name] = json.load(f)\n",
    "        print(f\"âœ… {name} chargÃ©\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ {name} non trouvÃ© (results/metrics/{model}_results.json)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ModÃ¨les chargÃ©s: {len(results_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a52cf",
   "metadata": {},
   "source": [
    "## ğŸ“Š 4. Tableau Comparatif Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce24adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame de comparaison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in results_dict.items():\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    row = {\n",
    "        'ModÃ¨le': model_name,\n",
    "        'F1 (micro)': metrics['f1_micro'],\n",
    "        'F1 (macro)': metrics['f1_macro'],\n",
    "        'Precision (micro)': metrics['precision_micro'],\n",
    "        'Precision (macro)': metrics['precision_macro'],\n",
    "        'Recall (micro)': metrics['recall_micro'],\n",
    "        'Recall (macro)': metrics['recall_macro'],\n",
    "        'Hamming Loss': metrics['hamming_loss'],\n",
    "        'Subset Accuracy': metrics['subset_accuracy'],\n",
    "        'Params (M)': results['total_params'] / 1_000_000,\n",
    "        'Temps (min)': results['training_time_minutes']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# CrÃ©er le DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Trier par F1 micro\n",
    "comparison_df = comparison_df.sort_values('F1 (micro)', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š TABLEAU COMPARATIF DES MODÃˆLES\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sauvegarder\n",
    "comparison_df.to_csv('results/comparison/models_comparison.csv', index=False)\n",
    "print(\"\\nâœ… Tableau sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89217da0",
   "metadata": {},
   "source": [
    "## ğŸ† 5. Meilleur ModÃ¨le et Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le meilleur modÃ¨le\n",
    "best_model = comparison_df.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ† MEILLEUR MODÃˆLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ModÃ¨le: {best_model['ModÃ¨le']}\")\n",
    "print(f\"F1-Score (micro): {best_model['F1 (micro)']:.4f}\")\n",
    "print(f\"F1-Score (macro): {best_model['F1 (macro)']:.4f}\")\n",
    "print(f\"Precision (micro): {best_model['Precision (micro)']:.4f}\")\n",
    "print(f\"Recall (micro): {best_model['Recall (micro)']:.4f}\")\n",
    "print(f\"Hamming Loss: {best_model['Hamming Loss']:.4f}\")\n",
    "print(f\"ParamÃ¨tres: {best_model['Params (M)']:.2f}M\")\n",
    "print(f\"Temps d'entraÃ®nement: {best_model['Temps (min)']:.2f} min\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# AmÃ©lioration par rapport au baseline (LSTM)\n",
    "if 'LSTM' in comparison_df['ModÃ¨le'].values:\n",
    "    lstm_f1 = comparison_df[comparison_df['ModÃ¨le'] == 'LSTM']['F1 (micro)'].values[0]\n",
    "    best_f1 = best_model['F1 (micro)']\n",
    "    improvement = ((best_f1 - lstm_f1) / lstm_f1) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ AmÃ©lioration par rapport au baseline LSTM: +{improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fd162",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6. Visualisations Comparatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc5dd8",
   "metadata": {},
   "source": [
    "### 6.1 Graphique en Barres - MÃ©triques Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer les donnÃ©es pour le graphique\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('F1 (micro)', 'F1-Score (Micro)'),\n",
    "    ('F1 (macro)', 'F1-Score (Macro)'),\n",
    "    ('Precision (micro)', 'Precision (Micro)'),\n",
    "    ('Recall (micro)', 'Recall (Micro)')\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # CrÃ©er le barplot\n",
    "    bars = ax.bar(comparison_df['ModÃ¨le'], comparison_df[metric], alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Colorer le meilleur en vert\n",
    "    max_idx = comparison_df[metric].idxmax()\n",
    "    bars[max_idx].set_color('green')\n",
    "    \n",
    "    # Configuration\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphique des mÃ©triques sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53675a14",
   "metadata": {},
   "source": [
    "### 6.2 Radar Chart - Vue d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# PrÃ©parer les donnÃ©es pour le radar chart\n",
    "categories = ['F1 (micro)', 'F1 (macro)', 'Precision (micro)', 'Recall (micro)']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Calculer les angles\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# CrÃ©er le plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Couleurs pour chaque modÃ¨le\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "# Tracer chaque modÃ¨le\n",
    "for idx, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "    values = [row[cat] for cat in categories]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['ModÃ¨le'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Configuration\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Radar Chart - Comparaison des ModÃ¨les', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Radar chart sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4238f",
   "metadata": {},
   "source": [
    "### 6.3 Trade-off ComplexitÃ© vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: ParamÃ¨tres vs F1-Score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot les points\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax.scatter(row['Params (M)'], row['F1 (micro)'], \n",
    "               s=500, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les labels\n",
    "    ax.annotate(row['ModÃ¨le'], \n",
    "                (row['Params (M)'], row['F1 (micro)']),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Configuration\n",
    "ax.set_xlabel('Nombre de ParamÃ¨tres (Millions)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Trade-off: ComplexitÃ© du ModÃ¨le vs Performance', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/complexity_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphique complexitÃ© vs performance sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa52781",
   "metadata": {},
   "source": [
    "### 6.4 Temps d'entraÃ®nement vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Temps vs F1-Score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot les points\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax.scatter(row['Temps (min)'], row['F1 (micro)'], \n",
    "               s=500, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les labels\n",
    "    ax.annotate(row['ModÃ¨le'], \n",
    "                (row['Temps (min)'], row['F1 (micro)']),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Configuration\n",
    "ax.set_xlabel('Temps d\\'entraÃ®nement (minutes)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Trade-off: Temps d\\'entraÃ®nement vs Performance', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/time_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphique temps vs performance sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb70281",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 7. Analyse par Classe (Top/Bottom Ã‰motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ˆ Analyse des performances par classe...\\n\")\n",
    "\n",
    "# Charger les rÃ©sultats par classe de chaque modÃ¨le\n",
    "per_class_results = {}\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    try:\n",
    "        df = pd.read_csv(f'results/metrics/{model}_per_class.csv')\n",
    "        per_class_results[name] = df\n",
    "        print(f\"âœ… {name} per-class chargÃ©\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ {name} per-class non trouvÃ©\")\n",
    "\n",
    "if len(per_class_results) > 0:\n",
    "    # Trouver les Ã©motions communes\n",
    "    first_model = list(per_class_results.values())[0]\n",
    "    emotions = first_model['Emotion'].tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {len(emotions)} Ã©motions analysÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e38ce",
   "metadata": {},
   "source": [
    "### 7.1 Top 10 et Bottom 10 Ã‰motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les F1-scores pour chaque Ã©motion\n",
    "emotion_comparison = pd.DataFrame({'Emotion': emotions})\n",
    "\n",
    "for model_name, df in per_class_results.items():\n",
    "    emotion_comparison[model_name] = df.set_index('Emotion').loc[emotions, 'F1-Score'].values\n",
    "\n",
    "# Calculer la moyenne\n",
    "emotion_comparison['Moyenne'] = emotion_comparison[model_names].mean(axis=1)\n",
    "\n",
    "# Top 10 Ã©motions\n",
    "top_10 = emotion_comparison.nlargest(10, 'Moyenne')\n",
    "\n",
    "print(\"\\nğŸ“Š TOP 10 Ã‰motions (Meilleures performances moyennes):\")\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Bottom 10 Ã©motions\n",
    "bottom_10 = emotion_comparison.nsmallest(10, 'Moyenne')\n",
    "\n",
    "print(\"\\nğŸ“Š BOTTOM 10 Ã‰motions (Pires performances moyennes):\")\n",
    "print(bottom_10.to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "emotion_comparison.sort_values('Moyenne', ascending=False).to_csv(\n",
    "    'results/comparison/emotion_comparison.csv', index=False\n",
    ")\n",
    "print(\"\\nâœ… Comparaison par Ã©motion sauvegardÃ©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41df22",
   "metadata": {},
   "source": [
    "### 7.2 Visualisation des Top/Bottom Ã‰motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38196f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser Top 10 et Bottom 10\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Top 10\n",
    "top_10_sorted = top_10.sort_values('Moyenne')\n",
    "x_pos = np.arange(len(top_10_sorted))\n",
    "width = 0.2\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    axes[0].barh(x_pos + idx * width, top_10_sorted[model_name], \n",
    "                 width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[0].set_yticks(x_pos + width * 1.5)\n",
    "axes[0].set_yticklabels(top_10_sorted['Emotion'])\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Top 10 Ã‰motions - Comparaison des ModÃ¨les', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom 10\n",
    "bottom_10_sorted = bottom_10.sort_values('Moyenne')\n",
    "x_pos = np.arange(len(bottom_10_sorted))\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    axes[1].barh(x_pos + idx * width, bottom_10_sorted[model_name], \n",
    "                 width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[1].set_yticks(x_pos + width * 1.5)\n",
    "axes[1].set_yticklabels(bottom_10_sorted['Emotion'])\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('Bottom 10 Ã‰motions - Comparaison des ModÃ¨les', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/top_bottom_emotions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphique top/bottom Ã©motions sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c4215",
   "metadata": {},
   "source": [
    "## ğŸ”¬ 8. Ã‰tude d'Ablation\n",
    "\n",
    "Analyser l'impact des diffÃ©rentes composantes architecturales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a236afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”¬ Ã‰TUDE D'ABLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparer les modÃ¨les pour voir l'impact des composantes\n",
    "if 'LSTM' in comparison_df['ModÃ¨le'].values:\n",
    "    lstm_f1 = comparison_df[comparison_df['ModÃ¨le'] == 'LSTM']['F1 (micro)'].values[0]\n",
    "    print(f\"\\nBaseline LSTM: {lstm_f1:.4f}\")\n",
    "    \n",
    "    # Impact de l'attention (BiLSTM vs LSTM)\n",
    "    if 'BiLSTM+Attention' in comparison_df['ModÃ¨le'].values:\n",
    "        bilstm_f1 = comparison_df[comparison_df['ModÃ¨le'] == 'BiLSTM+Attention']['F1 (micro)'].values[0]\n",
    "        attention_gain = bilstm_f1 - lstm_f1\n",
    "        print(f\"\\n+ BiLSTM + Attention: {bilstm_f1:.4f} (gain: +{attention_gain:.4f})\")\n",
    "    \n",
    "    # Impact du CNN (CNN-BiLSTM vs BiLSTM)\n",
    "    if 'CNN-BiLSTM+Attention' in comparison_df['ModÃ¨le'].values and 'BiLSTM+Attention' in comparison_df['ModÃ¨le'].values:\n",
    "        cnn_bilstm_f1 = comparison_df[comparison_df['ModÃ¨le'] == 'CNN-BiLSTM+Attention']['F1 (micro)'].values[0]\n",
    "        cnn_gain = cnn_bilstm_f1 - bilstm_f1\n",
    "        print(f\"+ CNN layers: {cnn_bilstm_f1:.4f} (gain: +{cnn_gain:.4f})\")\n",
    "    \n",
    "    # Impact de BERT\n",
    "    if 'BERT' in comparison_df['ModÃ¨le'].values:\n",
    "        bert_f1 = comparison_df[comparison_df['ModÃ¨le'] == 'BERT']['F1 (micro)'].values[0]\n",
    "        bert_gain = bert_f1 - lstm_f1\n",
    "        print(f\"+ BERT (Transformers): {bert_f1:.4f} (gain: +{bert_gain:.4f})\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualiser l'ablation\n",
    "ablation_data = []\n",
    "if 'LSTM' in comparison_df['ModÃ¨le'].values:\n",
    "    ablation_data.append(('LSTM\\n(Baseline)', lstm_f1))\n",
    "if 'BiLSTM+Attention' in comparison_df['ModÃ¨le'].values:\n",
    "    ablation_data.append(('+ BiLSTM\\n+ Attention', bilstm_f1))\n",
    "if 'CNN-BiLSTM+Attention' in comparison_df['ModÃ¨le'].values:\n",
    "    ablation_data.append(('+ CNN', cnn_bilstm_f1))\n",
    "if 'BERT' in comparison_df['ModÃ¨le'].values:\n",
    "    ablation_data.append(('BERT\\n(Pre-trained)', bert_f1))\n",
    "\n",
    "if len(ablation_data) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    labels, values = zip(*ablation_data)\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold'][:len(labels)]\n",
    "    \n",
    "    bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Ã‰tude d\\'Ablation - Impact des Composantes', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/comparison/ablation_study.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Graphique d'ablation sauvegardÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b109a",
   "metadata": {},
   "source": [
    "## ğŸ’¡ 9. Recommandations et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a330444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¡ RECOMMANDATIONS ET CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# GÃ©nÃ©rer les recommandations basÃ©es sur les rÃ©sultats\n",
    "recommendations = []\n",
    "\n",
    "# Meilleur modÃ¨le global\n",
    "best_model_name = comparison_df.iloc[0]['ModÃ¨le']\n",
    "recommendations.append(f\"ğŸ† Meilleur modÃ¨le global: {best_model_name}\")\n",
    "recommendations.append(f\"   F1-Score: {comparison_df.iloc[0]['F1 (micro)']:.4f}\")\n",
    "\n",
    "# ModÃ¨le le plus efficace (meilleur ratio performance/complexitÃ©)\n",
    "comparison_df['Efficiency'] = comparison_df['F1 (micro)'] / (comparison_df['Params (M)'] + 1)\n",
    "most_efficient = comparison_df.iloc[comparison_df['Efficiency'].idxmax()]\n",
    "recommendations.append(f\"\\nâš¡ ModÃ¨le le plus efficace: {most_efficient['ModÃ¨le']}\")\n",
    "recommendations.append(f\"   Ratio performance/complexitÃ©: {most_efficient['Efficiency']:.4f}\")\n",
    "\n",
    "# ModÃ¨le le plus rapide\n",
    "fastest_model = comparison_df.iloc[comparison_df['Temps (min)'].idxmin()]\n",
    "recommendations.append(f\"\\nğŸš€ ModÃ¨le le plus rapide: {fastest_model['ModÃ¨le']}\")\n",
    "recommendations.append(f\"   Temps: {fastest_model['Temps (min)']:.2f} minutes\")\n",
    "\n",
    "# Recommandations selon le cas d'usage\n",
    "recommendations.append(\"\\nğŸ“‹ Recommandations selon le cas d'usage:\")\n",
    "recommendations.append(\"\\n1. Production (haute performance requise):\")\n",
    "recommendations.append(f\"   â†’ {best_model_name}\")\n",
    "recommendations.append(\"   â†’ Meilleur F1-Score, acceptable en termes de ressources\")\n",
    "\n",
    "recommendations.append(\"\\n2. Environnement avec ressources limitÃ©es:\")\n",
    "recommendations.append(f\"   â†’ {most_efficient['ModÃ¨le']}\")\n",
    "recommendations.append(\"   â†’ Bon compromis performance/complexitÃ©\")\n",
    "\n",
    "recommendations.append(\"\\n3. Prototypage rapide / DÃ©veloppement:\")\n",
    "recommendations.append(f\"   â†’ {fastest_model['ModÃ¨le']}\")\n",
    "recommendations.append(\"   â†’ EntraÃ®nement rapide pour itÃ©rations\")\n",
    "\n",
    "# Insights sur les Ã©motions\n",
    "if len(per_class_results) > 0:\n",
    "    recommendations.append(\"\\nğŸ“Š Insights sur les Ã©motions:\")\n",
    "    recommendations.append(f\"   - Top Ã©motion: {top_10.iloc[0]['Emotion']} (F1: {top_10.iloc[0]['Moyenne']:.4f})\")\n",
    "    recommendations.append(f\"   - Ã‰motion difficile: {bottom_10.iloc[0]['Emotion']} (F1: {bottom_10.iloc[0]['Moyenne']:.4f})\")\n",
    "    recommendations.append(\"   â†’ Concentrer les amÃ©liorations sur les Ã©motions difficiles\")\n",
    "\n",
    "# Afficher les recommandations\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sauvegarder les recommandations\n",
    "with open('results/comparison/recommendations.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(recommendations))\n",
    "\n",
    "print(\"\\nâœ… Recommandations sauvegardÃ©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327140e0",
   "metadata": {},
   "source": [
    "## ğŸ“„ 10. GÃ©nÃ©ration du Rapport Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃ©nÃ©rer un rapport markdown complet\n",
    "report = f\"\"\"# ğŸ“Š Rapport Final - DÃ©tection d'Ã‰motions Multi-Label\n",
    "\n",
    "## 1. Vue d'ensemble du Projet\n",
    "\n",
    "**Dataset**: GoEmotions (28 classes d'Ã©motions)\n",
    "\n",
    "**ModÃ¨les entraÃ®nÃ©s**: {len(comparison_df)}\n",
    "- {', '.join(comparison_df['ModÃ¨le'].tolist())}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. RÃ©sultats Comparatifs\n",
    "\n",
    "### 2.1 Tableau des Performances\n",
    "\n",
    "{comparison_df.to_markdown(index=False)}\n",
    "\n",
    "### 2.2 Meilleur ModÃ¨le\n",
    "\n",
    "**ğŸ† {best_model_name}**\n",
    "- F1-Score (micro): {best_model['F1 (micro)']:.4f}\n",
    "- F1-Score (macro): {best_model['F1 (macro)']:.4f}\n",
    "- Precision (micro): {best_model['Precision (micro)']:.4f}\n",
    "- Recall (micro): {best_model['Recall (micro)']:.4f}\n",
    "- ParamÃ¨tres: {best_model['Params (M)']:.2f}M\n",
    "- Temps d'entraÃ®nement: {best_model['Temps (min)']:.2f} min\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Analyse des Ã‰motions\n",
    "\n",
    "### 3.1 Top 10 Ã‰motions (meilleures performances)\n",
    "\n",
    "{top_10[['Emotion', 'Moyenne']].to_markdown(index=False)}\n",
    "\n",
    "### 3.2 Bottom 10 Ã‰motions (performances Ã  amÃ©liorer)\n",
    "\n",
    "{bottom_10[['Emotion', 'Moyenne']].to_markdown(index=False)}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recommandations\n",
    "\n",
    "{chr(10).join(recommendations)}\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Fichiers GÃ©nÃ©rÃ©s\n",
    "\n",
    "- RÃ©sultats des modÃ¨les: `results/metrics/`\n",
    "- Graphiques: `results/figures/`\n",
    "- Comparaisons: `results/comparison/`\n",
    "\n",
    "---\n",
    "\n",
    "**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open('results/comparison/final_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"âœ… Rapport final gÃ©nÃ©rÃ©: results/comparison/final_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91271794",
   "metadata": {},
   "source": [
    "## ğŸ‰ 11. RÃ©sumÃ© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caac016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ ANALYSE COMPARATIVE TERMINÃ‰E!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… {len(comparison_df)} modÃ¨les comparÃ©s\")\n",
    "print(f\"âœ… {len(emotions) if len(per_class_results) > 0 else 'N/A'} Ã©motions analysÃ©es\")\n",
    "print(f\"âœ… {len([f for f in Path('results/comparison').glob('*.png')])} graphiques gÃ©nÃ©rÃ©s\")\n",
    "print(f\"\\nğŸ“‚ Tous les rÃ©sultats sont dans:\")\n",
    "print(f\"   - results/comparison/\")\n",
    "print(f\"   - results/figures/\")\n",
    "print(f\"   - results/metrics/\")\n",
    "\n",
    "print(\"\\nğŸ“Š Fichiers clÃ©s:\")\n",
    "print(\"   - models_comparison.csv: Tableau comparatif\")\n",
    "print(\"   - final_report.md: Rapport complet\")\n",
    "print(\"   - recommendations.txt: Recommandations\")\n",
    "print(\"   - *.png: Tous les graphiques\")\n",
    "\n",
    "print(\"\\nğŸ† Meilleur modÃ¨le: \" + best_model_name)\n",
    "print(f\"   F1-Score: {best_model['F1 (micro)']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ Projet de DÃ©tection d'Ã‰motions - COMPLET!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
