{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c59958",
   "metadata": {},
   "source": [
    "## üì¶ 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ab7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c8673",
   "metadata": {},
   "source": [
    "## üìÇ 2. Cr√©ation des Dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les dossiers si n√©cessaire\n",
    "folders = [\n",
    "    'results/figures',\n",
    "    'results/metrics',\n",
    "    'results/comparison'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"‚úÖ Dossiers cr√©√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab637c0",
   "metadata": {},
   "source": [
    "## üì• 3. Chargement des R√©sultats de Tous les Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Chargement des r√©sultats...\\n\")\n",
    "\n",
    "# Liste des mod√®les\n",
    "models = ['lstm', 'bilstm_attention', 'cnn_bilstm_attention', 'bert']\n",
    "model_names = ['LSTM', 'BiLSTM+Attention', 'CNN-BiLSTM+Attention', 'BERT']\n",
    "\n",
    "# Charger les r√©sultats\n",
    "results_dict = {}\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    try:\n",
    "        with open(f'results/metrics/{model}_results.json', 'r') as f:\n",
    "            results_dict[name] = json.load(f)\n",
    "        print(f\"‚úÖ {name} charg√©\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è {name} non trouv√© (results/metrics/{model}_results.json)\")\n",
    "\n",
    "print(f\"\\nüìä Mod√®les charg√©s: {len(results_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a52cf",
   "metadata": {},
   "source": [
    "## üìä 4. Tableau Comparatif Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce24adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame de comparaison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in results_dict.items():\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    row = {\n",
    "        'Mod√®le': model_name,\n",
    "        'F1 (micro)': metrics['f1_micro'],\n",
    "        'F1 (macro)': metrics['f1_macro'],\n",
    "        'Precision (micro)': metrics['precision_micro'],\n",
    "        'Precision (macro)': metrics['precision_macro'],\n",
    "        'Recall (micro)': metrics['recall_micro'],\n",
    "        'Recall (macro)': metrics['recall_macro'],\n",
    "        'Hamming Loss': metrics['hamming_loss'],\n",
    "        'Subset Accuracy': metrics['subset_accuracy'],\n",
    "        'Params (M)': results['total_params'] / 1_000_000,\n",
    "        'Temps (min)': results['training_time_minutes']\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Cr√©er le DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Trier par F1 micro\n",
    "comparison_df = comparison_df.sort_values('F1 (micro)', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä TABLEAU COMPARATIF DES MOD√àLES\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sauvegarder\n",
    "comparison_df.to_csv('results/comparison/models_comparison.csv', index=False)\n",
    "print(\"\\n‚úÖ Tableau sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89217da0",
   "metadata": {},
   "source": [
    "## üèÜ 5. Meilleur Mod√®le et Statistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488df3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le meilleur mod√®le\n",
    "best_model = comparison_df.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ MEILLEUR MOD√àLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mod√®le: {best_model['Mod√®le']}\")\n",
    "print(f\"F1-Score (micro): {best_model['F1 (micro)']:.4f}\")\n",
    "print(f\"F1-Score (macro): {best_model['F1 (macro)']:.4f}\")\n",
    "print(f\"Precision (micro): {best_model['Precision (micro)']:.4f}\")\n",
    "print(f\"Recall (micro): {best_model['Recall (micro)']:.4f}\")\n",
    "print(f\"Hamming Loss: {best_model['Hamming Loss']:.4f}\")\n",
    "print(f\"Param√®tres: {best_model['Params (M)']:.2f}M\")\n",
    "print(f\"Temps d'entra√Ænement: {best_model['Temps (min)']:.2f} min\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Am√©lioration par rapport au baseline (LSTM)\n",
    "if 'LSTM' in comparison_df['Mod√®le'].values:\n",
    "    lstm_f1 = comparison_df[comparison_df['Mod√®le'] == 'LSTM']['F1 (micro)'].values[0]\n",
    "    best_f1 = best_model['F1 (micro)']\n",
    "    improvement = ((best_f1 - lstm_f1) / lstm_f1) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Am√©lioration par rapport au baseline LSTM: +{improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fd162",
   "metadata": {},
   "source": [
    "## üìä 6. Visualisations Comparatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc5dd8",
   "metadata": {},
   "source": [
    "### 6.1 Graphique en Barres - M√©triques Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer les donn√©es pour le graphique\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('F1 (micro)', 'F1-Score (Micro)'),\n",
    "    ('F1 (macro)', 'F1-Score (Macro)'),\n",
    "    ('Precision (micro)', 'Precision (Micro)'),\n",
    "    ('Recall (micro)', 'Recall (Micro)')\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Cr√©er le barplot\n",
    "    bars = ax.bar(comparison_df['Mod√®le'], comparison_df[metric], alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Colorer le meilleur en vert\n",
    "    max_idx = comparison_df[metric].idxmax()\n",
    "    bars[max_idx].set_color('green')\n",
    "    \n",
    "    # Configuration\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique des m√©triques sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53675a14",
   "metadata": {},
   "source": [
    "### 6.2 Radar Chart - Vue d'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Pr√©parer les donn√©es pour le radar chart\n",
    "categories = ['F1 (micro)', 'F1 (macro)', 'Precision (micro)', 'Recall (micro)']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Calculer les angles\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Cr√©er le plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Couleurs pour chaque mod√®le\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "# Tracer chaque mod√®le\n",
    "for idx, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "    values = [row[cat] for cat in categories]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Mod√®le'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Configuration\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Radar Chart - Comparaison des Mod√®les', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Radar chart sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4238f",
   "metadata": {},
   "source": [
    "### 6.3 Trade-off Complexit√© vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Param√®tres vs F1-Score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot les points\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax.scatter(row['Params (M)'], row['F1 (micro)'], \n",
    "               s=500, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les labels\n",
    "    ax.annotate(row['Mod√®le'], \n",
    "                (row['Params (M)'], row['F1 (micro)']),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Configuration\n",
    "ax.set_xlabel('Nombre de Param√®tres (Millions)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Trade-off: Complexit√© du Mod√®le vs Performance', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/complexity_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique complexit√© vs performance sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa52781",
   "metadata": {},
   "source": [
    "### 6.4 Temps d'entra√Ænement vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Temps vs F1-Score\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot les points\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax.scatter(row['Temps (min)'], row['F1 (micro)'], \n",
    "               s=500, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les labels\n",
    "    ax.annotate(row['Mod√®le'], \n",
    "                (row['Temps (min)'], row['F1 (micro)']),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Configuration\n",
    "ax.set_xlabel('Temps d\\'entra√Ænement (minutes)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Trade-off: Temps d\\'entra√Ænement vs Performance', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/time_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique temps vs performance sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb70281",
   "metadata": {},
   "source": [
    "## üìà 7. Analyse par Classe (Top/Bottom √âmotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Analyse des performances par classe...\\n\")\n",
    "\n",
    "# Charger les r√©sultats par classe de chaque mod√®le\n",
    "per_class_results = {}\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    try:\n",
    "        df = pd.read_csv(f'results/metrics/{model}_per_class.csv')\n",
    "        per_class_results[name] = df\n",
    "        print(f\"‚úÖ {name} per-class charg√©\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è {name} per-class non trouv√©\")\n",
    "\n",
    "if len(per_class_results) > 0:\n",
    "    # Trouver les √©motions communes\n",
    "    first_model = list(per_class_results.values())[0]\n",
    "    emotions = first_model['Emotion'].tolist()\n",
    "    \n",
    "    print(f\"\\nüìä {len(emotions)} √©motions analys√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e38ce",
   "metadata": {},
   "source": [
    "### 7.1 Top 10 et Bottom 10 √âmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les F1-scores pour chaque √©motion\n",
    "emotion_comparison = pd.DataFrame({'Emotion': emotions})\n",
    "\n",
    "for model_name, df in per_class_results.items():\n",
    "    emotion_comparison[model_name] = df.set_index('Emotion').loc[emotions, 'F1-Score'].values\n",
    "\n",
    "# Calculer la moyenne\n",
    "emotion_comparison['Moyenne'] = emotion_comparison[model_names].mean(axis=1)\n",
    "\n",
    "# Top 10 √©motions\n",
    "top_10 = emotion_comparison.nlargest(10, 'Moyenne')\n",
    "\n",
    "print(\"\\nüìä TOP 10 √âmotions (Meilleures performances moyennes):\")\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Bottom 10 √©motions\n",
    "bottom_10 = emotion_comparison.nsmallest(10, 'Moyenne')\n",
    "\n",
    "print(\"\\nüìä BOTTOM 10 √âmotions (Pires performances moyennes):\")\n",
    "print(bottom_10.to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "emotion_comparison.sort_values('Moyenne', ascending=False).to_csv(\n",
    "    'results/comparison/emotion_comparison.csv', index=False\n",
    ")\n",
    "print(\"\\n‚úÖ Comparaison par √©motion sauvegard√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41df22",
   "metadata": {},
   "source": [
    "### 7.2 Visualisation des Top/Bottom √âmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38196f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser Top 10 et Bottom 10\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Top 10\n",
    "top_10_sorted = top_10.sort_values('Moyenne')\n",
    "x_pos = np.arange(len(top_10_sorted))\n",
    "width = 0.2\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    axes[0].barh(x_pos + idx * width, top_10_sorted[model_name], \n",
    "                 width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[0].set_yticks(x_pos + width * 1.5)\n",
    "axes[0].set_yticklabels(top_10_sorted['Emotion'])\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Top 10 √âmotions - Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom 10\n",
    "bottom_10_sorted = bottom_10.sort_values('Moyenne')\n",
    "x_pos = np.arange(len(bottom_10_sorted))\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    axes[1].barh(x_pos + idx * width, bottom_10_sorted[model_name], \n",
    "                 width, label=model_name, alpha=0.8)\n",
    "\n",
    "axes[1].set_yticks(x_pos + width * 1.5)\n",
    "axes[1].set_yticklabels(bottom_10_sorted['Emotion'])\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('Bottom 10 √âmotions - Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison/top_bottom_emotions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Graphique top/bottom √©motions sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c4215",
   "metadata": {},
   "source": [
    "## üî¨ 8. √âtude d'Ablation\n",
    "\n",
    "Analyser l'impact des diff√©rentes composantes architecturales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a236afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî¨ √âTUDE D'ABLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparer les mod√®les pour voir l'impact des composantes\n",
    "if 'LSTM' in comparison_df['Mod√®le'].values:\n",
    "    lstm_f1 = comparison_df[comparison_df['Mod√®le'] == 'LSTM']['F1 (micro)'].values[0]\n",
    "    print(f\"\\nBaseline LSTM: {lstm_f1:.4f}\")\n",
    "    \n",
    "    # Impact de l'attention (BiLSTM vs LSTM)\n",
    "    if 'BiLSTM+Attention' in comparison_df['Mod√®le'].values:\n",
    "        bilstm_f1 = comparison_df[comparison_df['Mod√®le'] == 'BiLSTM+Attention']['F1 (micro)'].values[0]\n",
    "        attention_gain = bilstm_f1 - lstm_f1\n",
    "        print(f\"\\n+ BiLSTM + Attention: {bilstm_f1:.4f} (gain: +{attention_gain:.4f})\")\n",
    "    \n",
    "    # Impact du CNN (CNN-BiLSTM vs BiLSTM)\n",
    "    if 'CNN-BiLSTM+Attention' in comparison_df['Mod√®le'].values and 'BiLSTM+Attention' in comparison_df['Mod√®le'].values:\n",
    "        cnn_bilstm_f1 = comparison_df[comparison_df['Mod√®le'] == 'CNN-BiLSTM+Attention']['F1 (micro)'].values[0]\n",
    "        cnn_gain = cnn_bilstm_f1 - bilstm_f1\n",
    "        print(f\"+ CNN layers: {cnn_bilstm_f1:.4f} (gain: +{cnn_gain:.4f})\")\n",
    "    \n",
    "    # Impact de BERT\n",
    "    if 'BERT' in comparison_df['Mod√®le'].values:\n",
    "        bert_f1 = comparison_df[comparison_df['Mod√®le'] == 'BERT']['F1 (micro)'].values[0]\n",
    "        bert_gain = bert_f1 - lstm_f1\n",
    "        print(f\"+ BERT (Transformers): {bert_f1:.4f} (gain: +{bert_gain:.4f})\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualiser l'ablation\n",
    "ablation_data = []\n",
    "if 'LSTM' in comparison_df['Mod√®le'].values:\n",
    "    ablation_data.append(('LSTM\\n(Baseline)', lstm_f1))\n",
    "if 'BiLSTM+Attention' in comparison_df['Mod√®le'].values:\n",
    "    ablation_data.append(('+ BiLSTM\\n+ Attention', bilstm_f1))\n",
    "if 'CNN-BiLSTM+Attention' in comparison_df['Mod√®le'].values:\n",
    "    ablation_data.append(('+ CNN', cnn_bilstm_f1))\n",
    "if 'BERT' in comparison_df['Mod√®le'].values:\n",
    "    ablation_data.append(('BERT\\n(Pre-trained)', bert_f1))\n",
    "\n",
    "if len(ablation_data) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    labels, values = zip(*ablation_data)\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold'][:len(labels)]\n",
    "    \n",
    "    bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('F1-Score (Micro)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('√âtude d\\'Ablation - Impact des Composantes', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/comparison/ablation_study.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Graphique d'ablation sauvegard√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b109a",
   "metadata": {},
   "source": [
    "## üí° 9. Recommandations et Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a330444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° RECOMMANDATIONS ET CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# G√©n√©rer les recommandations bas√©es sur les r√©sultats\n",
    "recommendations = []\n",
    "\n",
    "# Meilleur mod√®le global\n",
    "best_model_name = comparison_df.iloc[0]['Mod√®le']\n",
    "recommendations.append(f\"üèÜ Meilleur mod√®le global: {best_model_name}\")\n",
    "recommendations.append(f\"   F1-Score: {comparison_df.iloc[0]['F1 (micro)']:.4f}\")\n",
    "\n",
    "# Mod√®le le plus efficace (meilleur ratio performance/complexit√©)\n",
    "comparison_df['Efficiency'] = comparison_df['F1 (micro)'] / (comparison_df['Params (M)'] + 1)\n",
    "most_efficient = comparison_df.iloc[comparison_df['Efficiency'].idxmax()]\n",
    "recommendations.append(f\"\\n‚ö° Mod√®le le plus efficace: {most_efficient['Mod√®le']}\")\n",
    "recommendations.append(f\"   Ratio performance/complexit√©: {most_efficient['Efficiency']:.4f}\")\n",
    "\n",
    "# Mod√®le le plus rapide\n",
    "fastest_model = comparison_df.iloc[comparison_df['Temps (min)'].idxmin()]\n",
    "recommendations.append(f\"\\nüöÄ Mod√®le le plus rapide: {fastest_model['Mod√®le']}\")\n",
    "recommendations.append(f\"   Temps: {fastest_model['Temps (min)']:.2f} minutes\")\n",
    "\n",
    "# Recommandations selon le cas d'usage\n",
    "recommendations.append(\"\\nüìã Recommandations selon le cas d'usage:\")\n",
    "recommendations.append(\"\\n1. Production (haute performance requise):\")\n",
    "recommendations.append(f\"   ‚Üí {best_model_name}\")\n",
    "recommendations.append(\"   ‚Üí Meilleur F1-Score, acceptable en termes de ressources\")\n",
    "\n",
    "recommendations.append(\"\\n2. Environnement avec ressources limit√©es:\")\n",
    "recommendations.append(f\"   ‚Üí {most_efficient['Mod√®le']}\")\n",
    "recommendations.append(\"   ‚Üí Bon compromis performance/complexit√©\")\n",
    "\n",
    "recommendations.append(\"\\n3. Prototypage rapide / D√©veloppement:\")\n",
    "recommendations.append(f\"   ‚Üí {fastest_model['Mod√®le']}\")\n",
    "recommendations.append(\"   ‚Üí Entra√Ænement rapide pour it√©rations\")\n",
    "\n",
    "# Insights sur les √©motions\n",
    "if len(per_class_results) > 0:\n",
    "    recommendations.append(\"\\nüìä Insights sur les √©motions:\")\n",
    "    recommendations.append(f\"   - Top √©motion: {top_10.iloc[0]['Emotion']} (F1: {top_10.iloc[0]['Moyenne']:.4f})\")\n",
    "    recommendations.append(f\"   - √âmotion difficile: {bottom_10.iloc[0]['Emotion']} (F1: {bottom_10.iloc[0]['Moyenne']:.4f})\")\n",
    "    recommendations.append(\"   ‚Üí Concentrer les am√©liorations sur les √©motions difficiles\")\n",
    "\n",
    "# Afficher les recommandations\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sauvegarder les recommandations\n",
    "with open('results/comparison/recommendations.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(recommendations))\n",
    "\n",
    "print(\"\\n‚úÖ Recommandations sauvegard√©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327140e0",
   "metadata": {},
   "source": [
    "## üìÑ 10. G√©n√©ration du Rapport Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f77df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer un rapport markdown complet\n",
    "report = f\"\"\"# üìä Rapport Final - D√©tection d'√âmotions Multi-Label\n",
    "\n",
    "## 1. Vue d'ensemble du Projet\n",
    "\n",
    "**Dataset**: GoEmotions (28 classes d'√©motions)\n",
    "\n",
    "**Mod√®les entra√Æn√©s**: {len(comparison_df)}\n",
    "- {', '.join(comparison_df['Mod√®le'].tolist())}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. R√©sultats Comparatifs\n",
    "\n",
    "### 2.1 Tableau des Performances\n",
    "\n",
    "{comparison_df.to_markdown(index=False)}\n",
    "\n",
    "### 2.2 Meilleur Mod√®le\n",
    "\n",
    "**üèÜ {best_model_name}**\n",
    "- F1-Score (micro): {best_model['F1 (micro)']:.4f}\n",
    "- F1-Score (macro): {best_model['F1 (macro)']:.4f}\n",
    "- Precision (micro): {best_model['Precision (micro)']:.4f}\n",
    "- Recall (micro): {best_model['Recall (micro)']:.4f}\n",
    "- Param√®tres: {best_model['Params (M)']:.2f}M\n",
    "- Temps d'entra√Ænement: {best_model['Temps (min)']:.2f} min\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Analyse des √âmotions\n",
    "\n",
    "### 3.1 Top 10 √âmotions (meilleures performances)\n",
    "\n",
    "{top_10[['Emotion', 'Moyenne']].to_markdown(index=False)}\n",
    "\n",
    "### 3.2 Bottom 10 √âmotions (performances √† am√©liorer)\n",
    "\n",
    "{bottom_10[['Emotion', 'Moyenne']].to_markdown(index=False)}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Recommandations\n",
    "\n",
    "{chr(10).join(recommendations)}\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Fichiers G√©n√©r√©s\n",
    "\n",
    "- R√©sultats des mod√®les: `results/metrics/`\n",
    "- Graphiques: `results/figures/`\n",
    "- Comparaisons: `results/comparison/`\n",
    "\n",
    "---\n",
    "\n",
    "**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open('results/comparison/final_report.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"‚úÖ Rapport final g√©n√©r√©: results/comparison/final_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91271794",
   "metadata": {},
   "source": [
    "## üéâ 11. R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caac016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ANALYSE COMPARATIVE TERMIN√âE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ {len(comparison_df)} mod√®les compar√©s\")\n",
    "print(f\"‚úÖ {len(emotions) if len(per_class_results) > 0 else 'N/A'} √©motions analys√©es\")\n",
    "print(f\"‚úÖ {len([f for f in Path('results/comparison').glob('*.png')])} graphiques g√©n√©r√©s\")\n",
    "print(f\"\\nüìÇ Tous les r√©sultats sont dans:\")\n",
    "print(f\"   - results/comparison/\")\n",
    "print(f\"   - results/figures/\")\n",
    "print(f\"   - results/metrics/\")\n",
    "\n",
    "print(\"\\nüìä Fichiers cl√©s:\")\n",
    "print(\"   - models_comparison.csv: Tableau comparatif\")\n",
    "print(\"   - final_report.md: Rapport complet\")\n",
    "print(\"   - recommendations.txt: Recommandations\")\n",
    "print(\"   - *.png: Tous les graphiques\")\n",
    "\n",
    "print(\"\\nüèÜ Meilleur mod√®le: \" + best_model_name)\n",
    "print(f\"   F1-Score: {best_model['F1 (micro)']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ Projet de D√©tection d'√âmotions - COMPLET!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b6dc7",
   "metadata": {},
   "source": [
    "## üîç 11. Analyse d'Explicabilit√© (Partie 5)\n",
    "\n",
    "Utilisation de LIME pour expliquer les pr√©dictions du mod√®le sur des exemples concrets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de LIME\n",
    "!pip install -q lime\n",
    "\n",
    "print(\"‚úÖ LIME install√©!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "print(\"üîÆ D√©marrage de l'analyse d'explicabilit√© (LIME)...\")\n",
    "\n",
    "# 1. Charger les ressources n√©cessaires\n",
    "DATA_PATH = 'data/processed'\n",
    "MODEL_PATH = 'models/lstm/best_model.h5'\n",
    "\n",
    "try:\n",
    "    # Charger le tokenizer\n",
    "    with open(f'{DATA_PATH}/tokenizer.pkl', 'rb') as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    \n",
    "    # Charger les m√©tadonn√©es pour labels et max_len\n",
    "    with open(f'{DATA_PATH}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        classes = metadata['emotion_labels']\n",
    "        max_len = metadata['max_sequence_length']\n",
    "\n",
    "    # Charger le mod√®le LSTM (plus rapide pour l'inf√©rence LIME)\n",
    "    model = load_model(MODEL_PATH)\n",
    "    print(\"‚úÖ Mod√®le LSTM et ressources charg√©s\")\n",
    "    \n",
    "    # Fonction de pr√©diction pour LIME\n",
    "    def predict_proba(texts):\n",
    "        # Tokenization et padding\n",
    "        seqs = tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "        # Pr√©diction\n",
    "        return model.predict(padded)\n",
    "\n",
    "    # Initialiser l'explainer\n",
    "    explainer = LimeTextExplainer(class_names=classes)\n",
    "    \n",
    "    # Textes exemples √† expliquer\n",
    "    test_texts = [\n",
    "        \"I am so happy and excited about this new project! It feels amazing.\", # Joy/Excitement\n",
    "        \"This is absolutely terrible, I hate how they treated you.\", # Anger/Disgust\n",
    "        \"I'm really worried about the exam tomorrow, I feel sick.\", # Fear/Nervousness\n",
    "        \"Thank you so much for your help, I really appreciate it.\" # Gratitude\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß† Analyse de {len(test_texts)} exemples...\")\n",
    "\n",
    "    for idx, text in enumerate(test_texts):\n",
    "        print(f\"\\nüìù Exemple {idx+1}: '{text}'\")\n",
    "        \n",
    "        # G√©n√©rer l'explication (top 2 labels)\n",
    "        exp = explainer.explain_instance(text, predict_proba, num_features=6, top_labels=2)\n",
    "        \n",
    "        # Afficher les top classes pr√©dites\n",
    "        probs = predict_proba([text])[0]\n",
    "        top_indices = probs.argsort()[-3:][::-1]\n",
    "        print(\"  Top pr√©dictions:\")\n",
    "        for i in top_indices:\n",
    "            print(f\"  - {classes[i]}: {probs[i]:.4f}\")\n",
    "            \n",
    "        # Sauvegarder la visualisation HTML\n",
    "        exp.save_to_file(f'results/figures/lime_explanation_{idx+1}.html')\n",
    "        print(f\"  ‚úÖ Explication sauvegard√©e: results/figures/lime_explanation_{idx+1}.html\")\n",
    "        \n",
    "        # Afficher les features importantes (liste)\n",
    "        print(\"  Mots impactants:\")\n",
    "        for label_idx in exp.available_labels():\n",
    "            label_name = classes[label_idx]\n",
    "            print(f\"    Pour '{label_name}': {exp.as_list(label=label_idx)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur lors de l'analyse LIME: {e}\")\n",
    "    print(\"Assurez-vous que le mod√®le LSTM est bien entra√Æn√© et sauvegard√©.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
