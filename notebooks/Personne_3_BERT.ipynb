{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8726c9d",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Installation et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "!pip install -q transformers datasets\n",
    "\n",
    "print(\"âœ… Installation terminÃ©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    hamming_loss, accuracy_score, classification_report\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Seed pour reproductibilitÃ©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# VÃ©rifier la mÃ©moire GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu = tf.config.list_physical_devices('GPU')[0]\n",
    "    print(f\"GPU: {gpu}\")\n",
    "    \n",
    "print(\"âœ… Imports rÃ©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac84cb2",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 2. CrÃ©ation des Dossiers et Montage Google Drive (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db56e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les dossiers nÃ©cessaires\n",
    "folders = [\n",
    "    'data/processed',\n",
    "    'models/bert',\n",
    "    'results/figures',\n",
    "    'results/metrics'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"âœ… Dossiers crÃ©Ã©s!\")\n",
    "\n",
    "# Optionnel: Monter Google Drive pour sauvegarder les rÃ©sultats\n",
    "MOUNT_DRIVE = False  # Mettre Ã  True pour monter Google Drive\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive montÃ©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f65e8b",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 3. Chargement des DonnÃ©es Brutes\n",
    "\n",
    "âš ï¸ **Important**: BERT nÃ©cessite son propre tokenizer, donc on charge les textes bruts (pas les sÃ©quences prÃ©-tokenizÃ©es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84806ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Chargement des donnÃ©es brutes depuis Google Drive...\\n\")\n",
    "\n",
    "# DÃ©finir le chemin vers les donnÃ©es dans Drive\n",
    "DATA_PATH = '/content/drive/MyDrive/emotion_detection_project/processed'\n",
    "\n",
    "# Charger les textes bruts\n",
    "with open(f'{DATA_PATH}/texts_train.pkl', 'rb') as f:\n",
    "    texts_train = pickle.load(f)\n",
    "    \n",
    "with open(f'{DATA_PATH}/texts_val.pkl', 'rb') as f:\n",
    "    texts_val = pickle.load(f)\n",
    "    \n",
    "with open(f'{DATA_PATH}/texts_test.pkl', 'rb') as f:\n",
    "    texts_test = pickle.load(f)\n",
    "    \n",
    "print(\"âœ… Textes chargÃ©s\")\n",
    "\n",
    "# Charger les labels\n",
    "y_train = np.load(f'{DATA_PATH}/y_train.npy')\n",
    "y_val = np.load(f'{DATA_PATH}/y_val.npy')\n",
    "y_test = np.load(f'{DATA_PATH}/y_test.npy')\n",
    "print(\"âœ… Labels chargÃ©s\")\n",
    "\n",
    "# Charger les mÃ©tadonnÃ©es\n",
    "with open(f'{DATA_PATH}/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "NUM_CLASSES = metadata['num_classes']\n",
    "EMOTION_LABELS = metadata['emotion_labels']\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistiques:\")\n",
    "print(f\"  Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"  Train: {len(texts_train):,} | Val: {len(texts_val):,} | Test: {len(texts_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9048221",
   "metadata": {},
   "source": [
    "## ðŸ”¤ 4. Tokenization BERT\n",
    "\n",
    "BERT utilise WordPiece tokenization et nÃ©cessite des tokens spÃ©ciaux [CLS] et [SEP]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¤ Chargement du tokenizer BERT...\\n\")\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Configuration\n",
    "MAX_LENGTH = 128  # BERT peut gÃ©rer jusqu'Ã  512, mais 128 est suffisant ici\n",
    "\n",
    "print(f\"âœ… Tokenizer chargÃ©: bert-base-uncased\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”„ Tokenization des textes...\\n\")\n",
    "\n",
    "# Tokenizer les textes\n",
    "def tokenize_texts(texts, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenizer les textes avec BERT tokenizer\"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Tokenizer train\n",
    "print(\"Tokenization train...\")\n",
    "train_encodings = tokenize_texts(texts_train, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Tokenizer val\n",
    "print(\"Tokenization val...\")\n",
    "val_encodings = tokenize_texts(texts_val, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Tokenizer test\n",
    "print(\"Tokenization test...\")\n",
    "test_encodings = tokenize_texts(texts_test, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(\"\\nâœ… Tokenization terminÃ©e!\")\n",
    "print(f\"  Shape input_ids: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"  Shape attention_mask: {train_encodings['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc0f8b",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ 5. Construction du ModÃ¨le BERT\n",
    "\n",
    "Utilisation de TFBertModel + couche de classification personnalisÃ©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_model(num_classes, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    CrÃ©e un modÃ¨le BERT pour la classification multi-label\n",
    "    \n",
    "    Architecture:\n",
    "    - BERT base (prÃ©-entraÃ®nÃ©)\n",
    "    - Dropout\n",
    "    - Dense(num_classes, sigmoid)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Charger BERT prÃ©-entraÃ®nÃ©\n",
    "    bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Inputs\n",
    "    input_ids = keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # BERT encoding\n",
    "    bert_output = bert(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Prendre la sortie [CLS] (premier token)\n",
    "    cls_output = bert_output.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Classification head\n",
    "    x = keras.layers.Dropout(0.3)(cls_output)\n",
    "    outputs = keras.layers.Dense(num_classes, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # CrÃ©er le modÃ¨le\n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        outputs=outputs,\n",
    "        name='BERT_Emotion_Classifier'\n",
    "    )\n",
    "    \n",
    "    # Compiler\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"ðŸ—ï¸ Construction du modÃ¨le BERT...\\n\")\n",
    "model = create_bert_model(NUM_CLASSES, learning_rate=2e-5)\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()\n",
    "\n",
    "# Compter les paramÃ¨tres\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nðŸ“Š Total paramÃ¨tres: {total_params:,}\")\n",
    "print(f\"âš ï¸ Note: BERT base contient ~110M de paramÃ¨tres!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac66dd",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 6. Configuration des Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les callbacks\n",
    "callbacks = [\n",
    "    # Early stopping: arrÃªter si pas d'amÃ©lioration aprÃ¨s 3 epochs\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint: sauvegarder le meilleur modÃ¨le\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/bert/best_model',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_format='tf',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate: rÃ©duire le LR si plateau\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configurÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0294797",
   "metadata": {},
   "source": [
    "## ðŸš€ 7. EntraÃ®nement du ModÃ¨le\n",
    "\n",
    "â±ï¸ **Temps estimÃ©**: 50-60 minutes sur GPU  \n",
    "âš ï¸ **Attention**: TrÃ¨s lent sur CPU (>8h), utiliser GPU si possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a369f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ DÃ©but de l'entraÃ®nement BERT...\\n\")\n",
    "print(\"âš ï¸ Cela peut prendre du temps (50-60 min sur GPU)...\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# EntraÃ®ner le modÃ¨le\n",
    "history = model.fit(\n",
    "    {\n",
    "        'input_ids': train_encodings['input_ids'],\n",
    "        'attention_mask': train_encodings['attention_mask']\n",
    "    },\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask']\n",
    "        },\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=5,  # 5 epochs suffisent gÃ©nÃ©ralement pour BERT\n",
    "    batch_size=16,  # Petit batch size pour GPU memory\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nâœ… EntraÃ®nement terminÃ©!\")\n",
    "print(f\"â±ï¸ Temps d'entraÃ®nement: {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b2635",
   "metadata": {},
   "source": [
    "## ðŸ“Š 8. Visualisation de l'EntraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une figure avec 4 sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2, marker='o')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, marker='o')\n",
    "axes[0, 0].set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2, marker='o')\n",
    "axes[0, 1].set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2, marker='o')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2, marker='o')\n",
    "axes[1, 0].set_title('Precision Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2, marker='o')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, marker='o')\n",
    "axes[1, 1].set_title('Recall Evolution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/bert_training.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7b6b9",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 9. Ã‰valuation sur l'Ensemble de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Ã‰valuation sur l'ensemble de test...\\n\")\n",
    "\n",
    "# PrÃ©dictions\n",
    "y_pred_proba = model.predict(\n",
    "    {\n",
    "        'input_ids': test_encodings['input_ids'],\n",
    "        'attention_mask': test_encodings['attention_mask']\n",
    "    },\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculer les mÃ©triques\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "subset_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š RÃ‰SULTATS SUR L'ENSEMBLE DE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸŽ¯ MÃ©triques Globales:\")\n",
    "print(f\"  Precision (micro): {precision_micro:.4f}\")\n",
    "print(f\"  Precision (macro): {precision_macro:.4f}\")\n",
    "print(f\"  Recall (micro):    {recall_micro:.4f}\")\n",
    "print(f\"  Recall (macro):    {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score (micro):  {f1_micro:.4f}\")\n",
    "print(f\"  F1-Score (macro):  {f1_macro:.4f}\")\n",
    "print(f\"  Hamming Loss:      {hamming:.4f}\")\n",
    "print(f\"  Subset Accuracy:   {subset_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6403586",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 10. MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les mÃ©triques par classe\n",
    "precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# CrÃ©er un DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Emotion': EMOTION_LABELS,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class,\n",
    "    'Support': y_test.sum(axis=0)\n",
    "})\n",
    "\n",
    "# Trier par F1-Score\n",
    "metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š TOP 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š BOTTOM 10 Ã‰motions (par F1-Score):\")\n",
    "print(metrics_df.tail(10).to_string(index=False))\n",
    "\n",
    "# Sauvegarder\n",
    "metrics_df.to_csv('results/metrics/bert_per_class.csv', index=False)\n",
    "print(\"\\nâœ… MÃ©triques par classe sauvegardÃ©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858d565",
   "metadata": {},
   "source": [
    "## ðŸ“Š 11. Visualisation des MÃ©triques par Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les 15 meilleures et 15 pires Ã©motions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Top 15\n",
    "top_15 = metrics_df.head(15).sort_values('F1-Score')\n",
    "axes[0].barh(top_15['Emotion'], top_15['F1-Score'], color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[0].set_title('Top 15 Ã‰motions (Meilleures F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Bottom 15\n",
    "bottom_15 = metrics_df.tail(15).sort_values('F1-Score')\n",
    "axes[1].barh(bottom_15['Emotion'], bottom_15['F1-Score'], color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('F1-Score', fontsize=12)\n",
    "axes[1].set_title('Bottom 15 Ã‰motions (Pires F1-Scores)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/figures/bert_per_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Graphiques sauvegardÃ©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9af63",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 12. Sauvegarde des RÃ©sultats Complets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les rÃ©sultats JSON\n",
    "results = {\n",
    "    'model_name': 'BERT',\n",
    "    'model_version': 'bert-base-uncased',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'training_time_minutes': training_time / 60,\n",
    "    'total_params': int(total_params),\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'metrics': {\n",
    "        'precision_micro': float(precision_micro),\n",
    "        'precision_macro': float(precision_macro),\n",
    "        'recall_micro': float(recall_micro),\n",
    "        'recall_macro': float(recall_macro),\n",
    "        'f1_micro': float(f1_micro),\n",
    "        'f1_macro': float(f1_macro),\n",
    "        'hamming_loss': float(hamming),\n",
    "        'subset_accuracy': float(subset_acc)\n",
    "    },\n",
    "    'best_epoch': int(np.argmin(history.history['val_loss'])) + 1,\n",
    "    'best_val_loss': float(min(history.history['val_loss']))\n",
    "}\n",
    "\n",
    "with open('results/metrics/bert_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… RÃ©sultats sauvegardÃ©s dans results/metrics/bert_results.json\")\n",
    "\n",
    "# Sauvegarder l'historique\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('results/metrics/bert_history.csv', index=False)\n",
    "print(\"âœ… Historique sauvegardÃ©!\")\n",
    "\n",
    "# Sauvegarder les prÃ©dictions\n",
    "np.save('results/metrics/bert_predictions.npy', y_pred_proba)\n",
    "print(\"âœ… PrÃ©dictions sauvegardÃ©es!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ NOTEBOOK BERT TERMINÃ‰!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
