"""
Interface Streamlit pour la D√©tection d'√âmotions
Projet GoEmotions - 28 √âmotions
"""

import streamlit as st
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import pickle
import plotly.graph_objects as go
import plotly.express as px
import re
import os

# Configuration de la page
st.set_page_config(
    page_title="D√©tection d'√âmotions - GoEmotions",
    page_icon="üòä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Liste des 28 √©motions
EMOTIONS = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',
    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',
    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',
    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',
    'relief', 'remorse', 'sadness', 'surprise', 'neutral'
]

MAX_SEQUENCE_LENGTH = 128

# Fonction de nettoyage du texte
def clean_text(text):
    """Nettoie le texte comme dans le preprocessing"""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Chargement des mod√®les
@st.cache_resource
def load_tokenizers():
    """Charge les tokenizers (Keras et BERT)"""
    tokenizers = {}
    
    # 1. Keras Tokenizer (pour LSTM, BiLSTM, CNN)
    try:
        paths = [
            '../data/processed/tokenizer.pkl',
            'data/processed/tokenizer.pkl',
            'tokenizer.pkl'
        ]
        for path in paths:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    tokenizers['keras'] = pickle.load(f)
                break
        if 'keras' not in tokenizers:
            st.warning("‚ö†Ô∏è Tokenizer Keras non trouv√©.")
    except Exception as e:
        st.error(f"Erreur loading Keras tokenizer: {e}")

    # 2. BERT Tokenizer
    try:
        from transformers import BertTokenizer
        tokenizers['bert'] = BertTokenizer.from_pretrained('bert-base-uncased')
    except Exception as e:
        st.error(f"Erreur loading BERT tokenizer: {e}")
        
    return tokenizers

@st.cache_resource
def load_model(model_name):
    """Charge le mod√®le s√©lectionn√©"""
    # Chemins ajust√©s selon les notebooks
    model_paths = {
        'LSTM': ['../models/lstm/best_model.h5', 'models/lstm/best_model.h5'],
        'BiLSTM + Attention': ['../models/bilstm/best_model.h5', 'models/bilstm/best_model.h5'],
        'CNN-BiLSTM + Attention': ['../models/cnn_bilstm/best_model.h5', 'models/cnn_bilstm/best_model.h5'],
        'BERT': ['../models/bert/best_model', 'models/bert/best_model'] # SavedModel format (folder)
    }
    
    # Essayer de trouver le bon chemin
    selected_path = None
    if model_name in model_paths:
        for path in model_paths[model_name]:
            if os.path.exists(path):
                selected_path = path
                break
    
    if not selected_path:
        return None

    try:
        # Custom objects pour les couches personnalis√©es
        custom_objects = {}
        if 'Attention' in model_name:
            # D√©finir AttentionLayer si n√©cessaire (copie de la classe des notebooks)
            class AttentionLayer(keras.layers.Layer):
                def __init__(self, **kwargs):
                    super(AttentionLayer, self).__init__(**kwargs)
                def build(self, input_shape):
                    self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), initializer='glorot_uniform', trainable=True)
                    self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)
                    super(AttentionLayer, self).build(input_shape)
                def call(self, x):
                    e = keras.backend.tanh(keras.backend.dot(x, self.W) + self.b)
                    a = keras.backend.softmax(e, axis=1)
                    output = x * a
                    return keras.backend.sum(output, axis=1)
                def get_config(self):
                    return super(AttentionLayer, self).get_config()
            
            custom_objects['AttentionLayer'] = AttentionLayer

        # Chargement
        if model_name == 'BERT':
            # BERT est souvent sauvegard√© en TF SavedModel
            from transformers import TFBertModel
            custom_objects['TFBertModel'] = TFBertModel
            model = keras.models.load_model(selected_path, custom_objects=custom_objects)
        else:
            model = keras.models.load_model(selected_path, custom_objects=custom_objects)
            
        return model
    except Exception as e:
        st.error(f"‚ùå Erreur d√©tail√©e chargement {model_name}: {e}")
        return None

# Fonction de pr√©diction
def predict_emotions(text, model, tokenizers, model_name, top_k=10):
    """Pr√©dit les √©motions pour un texte donn√©"""
    cleaned_text = clean_text(text)
    
    # Pr√©paration sp√©cifique selon le mod√®le
    if model_name == 'BERT':
        tokenizer = tokenizers['bert']
        encoding = tokenizer(
            cleaned_text,
            max_length=MAX_SEQUENCE_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='tf'
        )
        inputs = {
            'input_ids': encoding['input_ids'],
            'attention_mask': encoding['attention_mask']
        }
        predictions = model.predict(inputs, verbose=0)[0] # BERT retourne souvent un tuple ou dict
        # Si le mod√®le retourne un objet TF, il faut extraire les logits/probs
        if isinstance(predictions, dict):
             predictions = predictions['output'] # Ajuster selon le nom de la couche de sortie
    else:
        # Mod√®les Keras standards
        tokenizer = tokenizers['keras']
        sequences = tokenizer.texts_to_sequences([cleaned_text])
        padded = keras.preprocessing.sequence.pad_sequences(
            sequences, 
            maxlen=MAX_SEQUENCE_LENGTH, 
            padding='post'
        )
        predictions = model.predict(padded, verbose=0)[0]
    
    # Top K √©motions
    top_indices = np.argsort(predictions)[-top_k:][::-1]
    top_emotions = [EMOTIONS[i] for i in top_indices]
    top_probs = [predictions[i] for i in top_indices]
    
    return top_emotions, top_probs, predictions

# Interface principale
def main():
    # Titre et description
    st.title("üé≠ D√©tecteur d'√âmotions - GoEmotions Dataset")
    st.markdown("""
    Cette application utilise des mod√®les de Deep Learning pour d√©tecter **28 √©motions** 
    diff√©rentes dans un texte. Bas√©e sur le dataset **GoEmotions** (58,000 commentaires Reddit).
    """)
    
    # Sidebar - Configuration
    st.sidebar.title("‚öôÔ∏è Configuration")
    
    model_choice = st.sidebar.selectbox(
        "S√©lectionner un mod√®le",
        ["LSTM", "BiLSTM + Attention", "CNN-BiLSTM + Attention", "BERT"]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìä Informations du Mod√®le")
    
    # Informations sur les mod√®les
    model_info = {
        "LSTM": {
            "description": "Mod√®le baseline avec LSTM simple",
            "params": "~500K param√®tres",
            "temps": "~15-20 min"
        },
        "BiLSTM + Attention": {
            "description": "BiLSTM avec m√©canisme d'attention",
            "params": "~800K param√®tres",
            "temps": "~30-40 min"
        },
        "CNN-BiLSTM + Attention": {
            "description": "Architecture hybride CNN + BiLSTM",
            "params": "~1M param√®tres",
            "temps": "~40-50 min"
        },
        "BERT": {
            "description": "Fine-tuning BERT-base-uncased",
            "params": "~110M param√®tres",
            "temps": "~50-60 min"
        }
    }
    
    st.sidebar.info(f"**{model_choice}**\n\n{model_info[model_choice]['description']}")
    st.sidebar.caption(f"‚ö° {model_info[model_choice]['params']}")
    
    # Chargement des ressources
    with st.spinner("Chargement des tokenizers..."):
        tokenizers = load_tokenizers()
    
    with st.spinner(f"Chargement du mod√®le {model_choice}..."):
        model = load_model(model_choice)
    
    if model is None:
        st.error("‚ùå Impossible de charger le mod√®le. V√©rifiez que les mod√®les sont entra√Æn√©s.")
        st.info("üí° **Instructions** : Ex√©cutez d'abord les notebooks d'entra√Ænement (Notebook_1 √† Notebook_4)")
        return
    
    st.success(f"‚úÖ Mod√®le {model_choice} charg√© avec succ√®s!")
    
    # Zone de saisie
    st.markdown("---")
    st.header("‚úçÔ∏è Entrez votre texte")
    
    # Exemples pr√©d√©finis
    examples = {
        "Exemple 1 (Joie)": "I'm so happy and excited about this amazing news!",
        "Exemple 2 (Col√®re)": "This is absolutely frustrating and makes me so angry!",
        "Exemple 3 (Tristesse)": "I feel so sad and disappointed about what happened.",
        "Exemple 4 (Surprise)": "Wow, I can't believe this happened! What a surprise!",
        "Exemple 5 (Peur)": "I'm really scared and nervous about the situation."
    }
    
    selected_example = st.selectbox("Ou choisir un exemple", ["---"] + list(examples.keys()))
    
    if selected_example != "---":
        default_text = examples[selected_example]
    else:
        default_text = ""
    
    user_input = st.text_area(
        "Texte √† analyser",
        value=default_text,
        height=150,
        placeholder="Entrez un texte en anglais (ex: I love this wonderful day!)",
        help="Le mod√®le fonctionne mieux avec du texte en anglais"
    )
    
    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        analyze_button = st.button("üîç Analyser", type="primary")
    with col2:
        clear_button = st.button("üóëÔ∏è Effacer")
    
    if clear_button:
        st.rerun()
    
    # Analyse
    if analyze_button and user_input.strip():
        with st.spinner("Analyse en cours..."):
            top_emotions, top_probs, all_predictions = predict_emotions(
                user_input, model, tokenizers, model_choice, top_k=10
            )
        
        # R√©sultats
        st.markdown("---")
        st.header("üìä R√©sultats de l'Analyse")
        
        # M√©triques principales
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                "ü•á √âmotion Principale", 
                top_emotions[0].capitalize(),
                f"{top_probs[0]:.1%}"
            )
        with col2:
            st.metric(
                "ü•à Deuxi√®me √âmotion", 
                top_emotions[1].capitalize(),
                f"{top_probs[1]:.1%}"
            )
        with col3:
            st.metric(
                "ü•â Troisi√®me √âmotion", 
                top_emotions[2].capitalize(),
                f"{top_probs[2]:.1%}"
            )
        
        # Visualisations
        tab1, tab2, tab3 = st.tabs(["üìä Top 10 √âmotions", "üéØ Toutes les √âmotions", "üìà Distribution"])
        
        with tab1:
            # Graphique en barres - Top 10
            fig = go.Figure(data=[
                go.Bar(
                    x=top_probs,
                    y=[e.capitalize() for e in top_emotions],
                    orientation='h',
                    marker=dict(
                        color=top_probs,
                        colorscale='Viridis',
                        showscale=True,
                        colorbar=dict(title="Probabilit√©")
                    ),
                    text=[f"{p:.1%}" for p in top_probs],
                    textposition='auto',
                )
            ])
            
            fig.update_layout(
                title="Top 10 √âmotions D√©tect√©es",
                xaxis_title="Probabilit√©",
                yaxis_title="√âmotion",
                height=500,
                showlegend=False
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            # Toutes les √©motions
            emotions_df = pd.DataFrame({
                '√âmotion': EMOTIONS,
                'Probabilit√©': all_predictions
            }).sort_values('Probabilit√©', ascending=False)
            
            fig = px.bar(
                emotions_df,
                x='Probabilit√©',
                y='√âmotion',
                orientation='h',
                color='Probabilit√©',
                color_continuous_scale='RdYlGn',
                title="Distribution Compl√®te des 28 √âmotions"
            )
            
            fig.update_layout(height=800)
            st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            # Radar chart
            fig = go.Figure()
            
            fig.add_trace(go.Scatterpolar(
                r=top_probs[:8],
                theta=[e.capitalize() for e in top_emotions[:8]],
                fill='toself',
                name='√âmotions'
            ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, max(top_probs)]
                    )
                ),
                showlegend=False,
                title="Radar Chart - Top 8 √âmotions",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # Tableau d√©taill√©
        st.markdown("---")
        st.subheader("üìã D√©tails des Pr√©dictions")
        
        results_df = pd.DataFrame({
            'Rang': range(1, 11),
            '√âmotion': [e.capitalize() for e in top_emotions],
            'Probabilit√©': [f"{p:.4f}" for p in top_probs],
            'Pourcentage': [f"{p:.2%}" for p in top_probs]
        })
        
        st.dataframe(results_df, use_container_width=True, hide_index=True)
        
        # Texte nettoy√©
        with st.expander("üßπ Voir le texte nettoy√©"):
            st.code(clean_text(user_input))
    
    elif analyze_button:
        st.warning("‚ö†Ô∏è Veuillez entrer un texte √† analyser.")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center'>
        <p>üéì Projet D√©tection d'√âmotions - Deep Learning</p>
        <p>üìö Dataset: GoEmotions (58,000 commentaires Reddit, 28 √©motions)</p>
        <p>üèõÔ∏è 3ING - Indexation et Recherche d'Information</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
